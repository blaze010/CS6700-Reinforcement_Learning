{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fdecfd1",
   "metadata": {},
   "source": [
    "# <center >CS6700: Reinforcement Learning\n",
    "## <center >Programming Assignment 2\n",
    "## <center> DQN \\& AC: Part 4 - AC Cartpole\n",
    "###  Submitted by: \n",
    "### Gautham Govind A: EE19B022\n",
    "### Vishnu Vinod: CS19B048 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862e549",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7dfa317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: setuptools in /home/vishnu/.local/lib/python3.8/site-packages (67.6.0)\n",
      "/bin/bash: center: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Installing packages for rendering the game on Colab\n",
    "'''\n",
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!apt-get update > /dev/null 2>&1\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1\n",
    "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "082e3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "import tensorflow as tf\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "import tensorflow_probability as tfp\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc75fa",
   "metadata": {},
   "source": [
    "## CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0889a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "\n",
    "print(state_shape, no_of_actions)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a25f7",
   "metadata": {},
   "source": [
    "## AC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ff95d",
   "metadata": {},
   "source": [
    "**Actor-Critic methods** learn both a policy $\\pi(a|s;\\theta)$ and a state-value function $v(s;w)$ simultaneously. The policy is referred to as the actor that suggests actions given a state. The estimated value function is referred to as the critic. It evaluates actions taken by the actor based on the given policy. In this exercise, both functions are approximated by feedforward neural networks. \n",
    "\n",
    "- The policy network is parametrized by $\\theta$ - it takes a state $s$ as input and outputs the probabilities $\\pi(a|s;\\theta)\\ \\forall\\ a$\n",
    "- The value network is parametrized by $w$ - it takes a state $s$ as input and outputs a scalar value associated with the state, i.e., $v(s;w)$\n",
    "- The loss function to be minimized at every step ($L_{tot}^{(t)}$) is a summation of two terms, as follows:\n",
    "$$L_{tot}^{(t)} = L_{actor}^{(t)} + L_{critic}^{(t)}$$\n",
    "where,\n",
    "$$L_{actor}^{(t)} = -\\log\\pi(a_t|s_t; \\theta)\\delta_t$$\n",
    "$$L_{critic}^{(t)} = \\delta_t^2$$\n",
    "\n",
    "- **NOTE: Here, weights of the first two hidden layers are shared by the policy and the value network**\n",
    "    - Default hidden layer sizes: [1024, 512]\n",
    "    - Output size of policy network: 2 (Softmax activation)\n",
    "    - Output size of value network: 1 (Linear activation)\n",
    "\n",
    "$$\\pi(a|s;\\theta) = \\phi_{\\theta}(a,s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d5cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Defining policy and value networkss\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, n_hidden1=1024, n_hidden2=512):\n",
    "        super(ActorCriticModel, self).__init__()\n",
    "\n",
    "        #Hidden Layer 1\n",
    "        self.fc1 = tf.keras.layers.Dense(n_hidden1, activation='relu')\n",
    "        #Hidden Layer 2\n",
    "        self.fc2 = tf.keras.layers.Dense(n_hidden2, activation='relu')\n",
    "        \n",
    "        #Output Layer for policy\n",
    "        self.pi_out = tf.keras.layers.Dense(action_size, activation='softmax')\n",
    "        #Output Layer for state-value\n",
    "        self.v_out = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state):\n",
    "        \"\"\"\n",
    "        Computes policy distribution and state-value for a given state\n",
    "        \"\"\"\n",
    "        layer1 = self.fc1(state)\n",
    "        layer2 = self.fc2(layer1)\n",
    "\n",
    "        pi = self.pi_out(layer2)       # policy network outputs\n",
    "        v = self.v_out(layer2)         # value network outputs\n",
    "\n",
    "        return pi, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14116632",
   "metadata": {},
   "source": [
    "## Agent 1: One-Step AC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857327e",
   "metadata": {},
   "source": [
    "- The single step TD error can be defined as follows:\n",
    "$$\\delta_t  = R_{t+1} + \\gamma v(s_{t+1};w) - v(s_t;w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "815881c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent1:\n",
    "    \"\"\"\n",
    "    Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, lr=1e-4, gamma=0.99, seed = 85, h1 = 1024, h2 = 512):\n",
    "        self.gamma = gamma\n",
    "        self.ac_model = ActorCriticModel(action_size, h1, h2)\n",
    "        self.ac_model.compile(tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Given a state, compute the policy distribution over all actions and sample one action\n",
    "    def sample_action(self, state):\n",
    "        pi,_ = self.ac_model(state)\n",
    "        action_probabilities = tfp.distributions.Categorical(probs=pi)\n",
    "        sample = action_probabilities.sample()\n",
    "        return int(sample.numpy()[0])\n",
    "\n",
    "    # actor loss\n",
    "    def actor_loss(self, action, pi, delta):\n",
    "        return -tf.math.log(pi[0,action]) * delta\n",
    "\n",
    "    # critic loss\n",
    "    def critic_loss(self,delta):\n",
    "        return delta**2\n",
    "\n",
    "    @tf.function\n",
    "    # For a given transition (s,a,s',r) update the paramters\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            pi, V_s = self.ac_model(state)\n",
    "            _, V_s_next = self.ac_model(next_state)\n",
    "\n",
    "            V_s = tf.squeeze(V_s)\n",
    "            V_s_next = tf.squeeze(V_s_next)\n",
    "            \n",
    "\n",
    "            # Equation for TD error\n",
    "            delta = reward + self.gamma*V_s_next - V_s\n",
    "            loss_a = self.actor_loss(action, pi, delta)\n",
    "            loss_c =self.critic_loss(delta)\n",
    "            loss_total = loss_a + loss_c\n",
    "\n",
    "        gradient = tape.gradient(loss_total, self.ac_model.trainable_variables)\n",
    "        self.ac_model.optimizer.apply_gradients(zip(gradient, self.ac_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4839c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepAC:\n",
    "    def __init__(self, env_name, eps = 1000, runs = 10, lr = 1e-4, h1 = 1024, h2 = 512):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state = self.env.reset()[0]\n",
    "        self.thresh = self.env.spec.reward_threshold\n",
    "        self.lr = lr\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "        \n",
    "        self.num_runs = runs\n",
    "        self.num_eps = eps\n",
    "        self.avg_steps = []\n",
    "        self.avg_reward = []\n",
    "        self.var_reward = []\n",
    "        \n",
    "        self.eps_to_solve = [self.num_eps]*self.num_runs\n",
    "        self.solved = False\n",
    "        \n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        \n",
    "    def train(self, verbose = True):\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        for run in range(self.num_runs):\n",
    "            agent = Agent1(lr = self.lr, action_size = self.env.action_space.n, h1 = self.h1, h2 = self.h2)\n",
    "            eps_rewards, eps_steps = [], []\n",
    "            self.solved = False\n",
    "            \n",
    "            if verbose: print('begin experiment: RUN', run+1)\n",
    "                \n",
    "            for ep in tqdm(range(self.num_eps)):\n",
    "                steps, rewards = 0, 0\n",
    "                state = self.env.reset()[0].reshape(1,-1)\n",
    "                done = False\n",
    "                \n",
    "                while not done:\n",
    "                    action = agent.sample_action(state)\n",
    "                    n_state, rew, done, info, _ = self.env.step(action)\n",
    "                    n_state = n_state.reshape(1,-1)\n",
    "                    \n",
    "                    agent.learn(state, action, rew, n_state, done)\n",
    "                    state = n_state\n",
    "                    \n",
    "                    rewards += rew\n",
    "                    steps += 1\n",
    "                \n",
    "                eps_rewards.append(rewards)\n",
    "                eps_steps.append(steps)\n",
    "\n",
    "                if ep%10==0 and verbose: \n",
    "                    print('Ep', ep, 'Avg Reward: %f'%np.mean(eps_rewards[-10:]), 'Steps: %.1f'%eps_steps[-1])\n",
    "                    \n",
    "                if ep%100 and not self.solved:\n",
    "                    avg_100 =  np.mean(eps_rewards[-100:])\n",
    "                    if avg_100 > self.thresh:\n",
    "                        self.eps_to_solve[run] = ep - 100 if ep > 100 else ep\n",
    "                        self.solved = True\n",
    "                        \n",
    "            if verbose: \n",
    "                print('Solved': self.solved)\n",
    "                if self.solved:\n",
    "                    print('Environment Solved at Episode:',self.eps_to_solve[run])\n",
    "                    print('Env. Threshold:', self.thresh)\n",
    "                    print('Avg reward at Episode:', self.eps_to_solve[run], '')\n",
    "                print('End experiment: RUN', run+1)\n",
    "        \n",
    "            self.avg_reward.append(eps_rewards)\n",
    "            self.avg_steps.append(eps_steps)\n",
    "            \n",
    "        self.end_time = datetime.datetime.now()\n",
    "        if verbose: print('Time Taken', self.end_time - self.start_time)\n",
    "            \n",
    "    def plot_results(self):\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        reward_list = np.mean(self.avg_reward, axis = 0)\n",
    "        avg100_reward = np.array([np.mean(reward_list[max(0,i-100):i]) for i in range(1,len(reward_list)+1)])\n",
    "        \n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Number of Steps to Goal')\n",
    "        plt.title('Steps vs Episodes: Avg Steps: %.3f'%np.mean(self.avg_steps))\n",
    "        plt.plot(np.arange(self.num_eps), np.mean(self.avg_steps, axis = 0), 'b')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Episode Reward')\n",
    "        plt.title('Rewards vs Episodes: Avg Reward: %.3f'%np.mean(self.avg_reward))\n",
    "        plt.plot(np.arange(self.num_eps), reward_list, 'b')\n",
    "        plt.plot(np.arange(self.num_eps), avg100_reward, linewidth=2.5, 'r')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Variance by Episode')\n",
    "        plt.title('Variance across Runs')\n",
    "        plt.plot(np.arange(self.num_eps), np.var(self.avg_reward, axis = 0), 'b')\n",
    "        plt.show()\n",
    "        \n",
    "        return self.avg_steps, self.avg_reward, self.eps_to_solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c0d328",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c82be8",
   "metadata": {},
   "source": [
    "In order to obtain the best results we need to tune the following hyperparameters:\n",
    "\n",
    "- **Learning Rate ($\\alpha$)**:\n",
    "The learning rate controls how fast the model will learn. A learning rate that is too small will prove to be extremely slow while learning and is generally not useful. Similarly, a learning rate that is too high will cause huge jumps in values causing the model to overshoot the optimum parameters in the multi-dimensional parameter space. This will cause unpredictable and oscillatory behaviour of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "20967341",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = OneStepAC(env_name = 'CartPole-v1', )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1850fffc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin experment: RUN 1\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Agent1.learn at 0x7f834009edc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Ep 0 Avg Reward: 16.000000 Steps: 16.0\n",
      "Ep 10 Avg Reward: 21.200000 Steps: 10.0\n",
      "Ep 20 Avg Reward: 37.500000 Steps: 42.0\n",
      "Ep 30 Avg Reward: 61.300000 Steps: 111.0\n",
      "Ep 40 Avg Reward: 76.400000 Steps: 111.0\n",
      "Ep 50 Avg Reward: 93.700000 Steps: 90.0\n",
      "Ep 60 Avg Reward: 109.000000 Steps: 146.0\n",
      "Ep 70 Avg Reward: 120.800000 Steps: 97.0\n",
      "Ep 80 Avg Reward: 81.200000 Steps: 88.0\n",
      "Ep 90 Avg Reward: 79.600000 Steps: 54.0\n",
      "Ep 100 Avg Reward: 63.500000 Steps: 87.0\n",
      "Ep 110 Avg Reward: 77.000000 Steps: 91.0\n",
      "Ep 120 Avg Reward: 108.800000 Steps: 225.0\n",
      "Ep 130 Avg Reward: 82.100000 Steps: 99.0\n",
      "Ep 140 Avg Reward: 165.200000 Steps: 232.0\n",
      "Ep 150 Avg Reward: 168.800000 Steps: 183.0\n",
      "Ep 160 Avg Reward: 637.200000 Steps: 178.0\n",
      "Ep 170 Avg Reward: 191.500000 Steps: 176.0\n",
      "Ep 180 Avg Reward: 417.900000 Steps: 136.0\n",
      "Ep 190 Avg Reward: 584.200000 Steps: 220.0\n",
      "Ep 200 Avg Reward: 242.100000 Steps: 101.0\n",
      "Ep 210 Avg Reward: 104.700000 Steps: 98.0\n",
      "Ep 220 Avg Reward: 86.300000 Steps: 87.0\n",
      "Ep 230 Avg Reward: 90.200000 Steps: 95.0\n",
      "Ep 240 Avg Reward: 107.000000 Steps: 99.0\n",
      "Ep 250 Avg Reward: 82.400000 Steps: 96.0\n",
      "Ep 260 Avg Reward: 67.400000 Steps: 52.0\n",
      "Ep 270 Avg Reward: 117.200000 Steps: 115.0\n",
      "Ep 280 Avg Reward: 126.300000 Steps: 179.0\n",
      "Ep 290 Avg Reward: 192.500000 Steps: 356.0\n",
      "Ep 300 Avg Reward: 283.600000 Steps: 25.0\n",
      "Ep 310 Avg Reward: 19.700000 Steps: 20.0\n",
      "Ep 320 Avg Reward: 17.500000 Steps: 15.0\n",
      "Ep 330 Avg Reward: 14.600000 Steps: 13.0\n",
      "Ep 340 Avg Reward: 12.700000 Steps: 12.0\n",
      "Ep 350 Avg Reward: 14.800000 Steps: 15.0\n",
      "Ep 360 Avg Reward: 19.300000 Steps: 22.0\n",
      "Ep 370 Avg Reward: 23.400000 Steps: 22.0\n",
      "Ep 380 Avg Reward: 23.300000 Steps: 24.0\n",
      "Ep 390 Avg Reward: 22.800000 Steps: 25.0\n",
      "Ep 400 Avg Reward: 128.700000 Steps: 485.0\n",
      "Ep 410 Avg Reward: 426.700000 Steps: 177.0\n",
      "Ep 420 Avg Reward: 241.100000 Steps: 218.0\n",
      "Ep 430 Avg Reward: 360.800000 Steps: 204.0\n",
      "Ep 440 Avg Reward: 120.100000 Steps: 111.0\n",
      "Ep 450 Avg Reward: 76.500000 Steps: 121.0\n",
      "Ep 460 Avg Reward: 52.900000 Steps: 26.0\n",
      "Ep 470 Avg Reward: 28.600000 Steps: 25.0\n",
      "Ep 480 Avg Reward: 24.300000 Steps: 19.0\n",
      "Ep 490 Avg Reward: 23.900000 Steps: 24.0\n",
      "Ep 500 Avg Reward: 25.100000 Steps: 31.0\n",
      "Ep 510 Avg Reward: 30.000000 Steps: 33.0\n",
      "Ep 520 Avg Reward: 32.600000 Steps: 25.0\n",
      "Ep 530 Avg Reward: 66.800000 Steps: 97.0\n",
      "Ep 540 Avg Reward: 38.900000 Steps: 27.0\n",
      "Ep 550 Avg Reward: 34.000000 Steps: 38.0\n",
      "Ep 560 Avg Reward: 37.000000 Steps: 34.0\n",
      "Ep 570 Avg Reward: 32.700000 Steps: 34.0\n",
      "Ep 580 Avg Reward: 28.800000 Steps: 32.0\n",
      "Ep 590 Avg Reward: 22.400000 Steps: 23.0\n",
      "Ep 600 Avg Reward: 22.100000 Steps: 20.0\n",
      "Ep 610 Avg Reward: 21.100000 Steps: 19.0\n",
      "Ep 620 Avg Reward: 23.100000 Steps: 20.0\n",
      "Ep 630 Avg Reward: 20.700000 Steps: 20.0\n",
      "Ep 640 Avg Reward: 17.700000 Steps: 18.0\n",
      "Ep 650 Avg Reward: 22.700000 Steps: 24.0\n",
      "Ep 660 Avg Reward: 19.300000 Steps: 21.0\n",
      "Ep 670 Avg Reward: 19.400000 Steps: 18.0\n",
      "Ep 680 Avg Reward: 17.100000 Steps: 22.0\n",
      "Ep 690 Avg Reward: 18.600000 Steps: 19.0\n",
      "Ep 700 Avg Reward: 17.500000 Steps: 14.0\n",
      "Ep 710 Avg Reward: 17.700000 Steps: 22.0\n",
      "Ep 720 Avg Reward: 17.300000 Steps: 17.0\n",
      "Ep 730 Avg Reward: 14.700000 Steps: 16.0\n",
      "Ep 740 Avg Reward: 12.900000 Steps: 12.0\n",
      "Ep 750 Avg Reward: 13.100000 Steps: 13.0\n",
      "Ep 760 Avg Reward: 13.400000 Steps: 13.0\n",
      "Ep 770 Avg Reward: 16.400000 Steps: 17.0\n",
      "Ep 780 Avg Reward: 17.800000 Steps: 20.0\n",
      "Ep 790 Avg Reward: 18.300000 Steps: 19.0\n",
      "Ep 800 Avg Reward: 16.100000 Steps: 13.0\n",
      "Ep 810 Avg Reward: 15.400000 Steps: 15.0\n",
      "Ep 820 Avg Reward: 13.500000 Steps: 14.0\n",
      "Ep 830 Avg Reward: 13.600000 Steps: 14.0\n",
      "Ep 840 Avg Reward: 13.600000 Steps: 14.0\n",
      "Ep 850 Avg Reward: 14.600000 Steps: 13.0\n",
      "Ep 860 Avg Reward: 13.800000 Steps: 13.0\n",
      "Ep 870 Avg Reward: 15.300000 Steps: 16.0\n",
      "Ep 880 Avg Reward: 18.200000 Steps: 20.0\n",
      "Ep 890 Avg Reward: 19.300000 Steps: 17.0\n",
      "Ep 900 Avg Reward: 16.600000 Steps: 17.0\n",
      "Ep 910 Avg Reward: 18.700000 Steps: 21.0\n",
      "Ep 920 Avg Reward: 19.800000 Steps: 21.0\n",
      "Ep 930 Avg Reward: 22.400000 Steps: 20.0\n",
      "Ep 940 Avg Reward: 25.300000 Steps: 30.0\n",
      "Ep 950 Avg Reward: 31.600000 Steps: 33.0\n",
      "Ep 960 Avg Reward: 37.100000 Steps: 54.0\n",
      "Ep 970 Avg Reward: 59.200000 Steps: 103.0\n",
      "Ep 980 Avg Reward: 103.400000 Steps: 82.0\n",
      "Ep 990 Avg Reward: 84.500000 Steps: 94.0\n",
      "end experment: RUN 1\n",
      "begin experment: RUN 2\n",
      "Ep 0 Avg Reward: 16.000000 Steps: 16.0\n",
      "Ep 10 Avg Reward: 28.900000 Steps: 53.0\n",
      "Ep 20 Avg Reward: 67.800000 Steps: 35.0\n",
      "Ep 30 Avg Reward: 80.200000 Steps: 81.0\n",
      "Ep 40 Avg Reward: 47.300000 Steps: 37.0\n",
      "Ep 50 Avg Reward: 54.100000 Steps: 52.0\n",
      "Ep 60 Avg Reward: 64.200000 Steps: 43.0\n",
      "Ep 70 Avg Reward: 57.100000 Steps: 39.0\n",
      "Ep 80 Avg Reward: 61.600000 Steps: 55.0\n",
      "Ep 90 Avg Reward: 84.900000 Steps: 141.0\n",
      "Ep 100 Avg Reward: 121.700000 Steps: 184.0\n",
      "Ep 110 Avg Reward: 164.000000 Steps: 105.0\n",
      "Ep 120 Avg Reward: 117.300000 Steps: 159.0\n",
      "Ep 130 Avg Reward: 122.900000 Steps: 101.0\n",
      "Ep 140 Avg Reward: 113.500000 Steps: 110.0\n",
      "Ep 150 Avg Reward: 140.600000 Steps: 126.0\n",
      "Ep 160 Avg Reward: 196.300000 Steps: 125.0\n",
      "Ep 170 Avg Reward: 122.900000 Steps: 114.0\n",
      "Ep 180 Avg Reward: 159.700000 Steps: 196.0\n",
      "Ep 190 Avg Reward: 266.700000 Steps: 222.0\n",
      "Ep 200 Avg Reward: 235.300000 Steps: 402.0\n",
      "Ep 210 Avg Reward: 1121.200000 Steps: 26.0\n",
      "Ep 220 Avg Reward: 23.900000 Steps: 22.0\n",
      "Ep 230 Avg Reward: 29.500000 Steps: 18.0\n",
      "Ep 240 Avg Reward: 440.100000 Steps: 2386.0\n",
      "Ep 250 Avg Reward: 37.100000 Steps: 20.0\n",
      "Ep 260 Avg Reward: 19.700000 Steps: 18.0\n",
      "Ep 270 Avg Reward: 20.500000 Steps: 24.0\n",
      "Ep 280 Avg Reward: 23.000000 Steps: 26.0\n",
      "Ep 290 Avg Reward: 244.700000 Steps: 35.0\n",
      "Ep 300 Avg Reward: 28.000000 Steps: 19.0\n",
      "Ep 310 Avg Reward: 60.600000 Steps: 108.0\n",
      "Ep 320 Avg Reward: 122.900000 Steps: 157.0\n",
      "Ep 330 Avg Reward: 119.200000 Steps: 137.0\n",
      "Ep 340 Avg Reward: 83.400000 Steps: 94.0\n",
      "Ep 350 Avg Reward: 80.200000 Steps: 82.0\n",
      "Ep 360 Avg Reward: 86.500000 Steps: 60.0\n",
      "Ep 370 Avg Reward: 59.500000 Steps: 43.0\n",
      "Ep 380 Avg Reward: 58.100000 Steps: 40.0\n",
      "Ep 390 Avg Reward: 54.600000 Steps: 53.0\n",
      "Ep 400 Avg Reward: 40.300000 Steps: 46.0\n",
      "Ep 410 Avg Reward: 36.200000 Steps: 35.0\n",
      "Ep 420 Avg Reward: 39.500000 Steps: 46.0\n",
      "Ep 430 Avg Reward: 41.900000 Steps: 39.0\n",
      "Ep 440 Avg Reward: 37.600000 Steps: 35.0\n",
      "Ep 450 Avg Reward: 33.600000 Steps: 26.0\n",
      "Ep 460 Avg Reward: 30.500000 Steps: 26.0\n",
      "Ep 470 Avg Reward: 30.300000 Steps: 36.0\n",
      "Ep 480 Avg Reward: 30.900000 Steps: 31.0\n",
      "Ep 490 Avg Reward: 35.400000 Steps: 29.0\n",
      "Ep 500 Avg Reward: 39.500000 Steps: 48.0\n",
      "Ep 510 Avg Reward: 40.300000 Steps: 57.0\n",
      "Ep 520 Avg Reward: 43.800000 Steps: 29.0\n",
      "Ep 530 Avg Reward: 33.600000 Steps: 37.0\n",
      "Ep 540 Avg Reward: 31.800000 Steps: 36.0\n",
      "Ep 550 Avg Reward: 30.800000 Steps: 35.0\n",
      "Ep 560 Avg Reward: 30.000000 Steps: 26.0\n",
      "Ep 570 Avg Reward: 31.400000 Steps: 30.0\n",
      "Ep 580 Avg Reward: 26.700000 Steps: 26.0\n",
      "Ep 590 Avg Reward: 34.900000 Steps: 35.0\n",
      "Ep 600 Avg Reward: 46.500000 Steps: 39.0\n",
      "Ep 610 Avg Reward: 38.100000 Steps: 29.0\n",
      "Ep 620 Avg Reward: 31.900000 Steps: 34.0\n",
      "Ep 630 Avg Reward: 28.400000 Steps: 26.0\n",
      "Ep 640 Avg Reward: 26.500000 Steps: 23.0\n",
      "Ep 650 Avg Reward: 27.400000 Steps: 29.0\n",
      "Ep 660 Avg Reward: 29.000000 Steps: 27.0\n",
      "Ep 670 Avg Reward: 32.900000 Steps: 28.0\n",
      "Ep 680 Avg Reward: 29.200000 Steps: 29.0\n",
      "Ep 690 Avg Reward: 25.300000 Steps: 24.0\n",
      "Ep 700 Avg Reward: 20.100000 Steps: 20.0\n",
      "Ep 710 Avg Reward: 23.000000 Steps: 26.0\n",
      "Ep 720 Avg Reward: 27.400000 Steps: 27.0\n",
      "Ep 730 Avg Reward: 30.300000 Steps: 26.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[76], line 38\u001b[0m, in \u001b[0;36mOneStepAC.train\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     35\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 38\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     n_state, rew, done, info, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     40\u001b[0m     n_state \u001b[38;5;241m=\u001b[39m n_state\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 15\u001b[0m, in \u001b[0;36mAgent1.sample_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     13\u001b[0m pi,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mac_model(state)\n\u001b[1;32m     14\u001b[0m action_probabilities \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(probs\u001b[38;5;241m=\u001b[39mpi)\n\u001b[0;32m---> 15\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43maction_probabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(sample\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:1205\u001b[0m, in \u001b[0;36mDistribution.sample\u001b[0;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate samples of the specified shape.\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03mNote that a call to `sample()` without arguments will generate a single\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03m  samples: a `Tensor` with prepended dimensions `sample_shape`.\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_and_control_scope(name):\n\u001b[0;32m-> 1205\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_sample_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:1182\u001b[0m, in \u001b[0;36mDistribution._call_sample_n\u001b[0;34m(self, sample_shape, seed, **kwargs)\u001b[0m\n\u001b[1;32m   1178\u001b[0m sample_shape \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mconvert_to_shape_tensor(\n\u001b[1;32m   1179\u001b[0m     ps\u001b[38;5;241m.\u001b[39mcast(sample_shape, tf\u001b[38;5;241m.\u001b[39mint32), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1180\u001b[0m sample_shape, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_sample_shape_to_vector(\n\u001b[1;32m   1181\u001b[0m     sample_shape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1182\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_n\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m samples \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: tf\u001b[38;5;241m.\u001b[39mreshape(x, ps\u001b[38;5;241m.\u001b[39mconcat([sample_shape, ps\u001b[38;5;241m.\u001b[39mshape(x)[\u001b[38;5;241m1\u001b[39m:]], \u001b[38;5;241m0\u001b[39m)),\n\u001b[1;32m   1186\u001b[0m     samples)\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_sample_static_shape(samples, sample_shape, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_probability/python/distributions/categorical.py:250\u001b[0m, in \u001b[0;36mCategorical._sample_n\u001b[0;34m(self, n, seed)\u001b[0m\n\u001b[1;32m    245\u001b[0m   draws \u001b[38;5;241m=\u001b[39m samplers\u001b[38;5;241m.\u001b[39mcategorical(\n\u001b[1;32m    246\u001b[0m       logits_2d, n, dtype\u001b[38;5;241m=\u001b[39msample_dtype, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m    247\u001b[0m draws \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(draws, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    249\u001b[0m     tf\u001b[38;5;241m.\u001b[39mtranspose(draws),\n\u001b[0;32m--> 250\u001b[0m     shape\u001b[38;5;241m=\u001b[39m\u001b[43mps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_shape_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m<decorator-gen-48>:2\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/prefer_static.py:74\u001b[0m, in \u001b[0;36m_prefer_static.<locals>.wrap\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_static:\n\u001b[1;32m     73\u001b[0m   [args_, kwargs_] \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as([args, kwargs], flat_args_)\n\u001b[0;32m---> 74\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstatic_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m original_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/numpy/_utils.py:62\u001b[0m, in \u001b[0;36mcopy_docstring.<locals>.wrap\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@wrapt\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(wrapped, instance, args, kwargs):\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m instance, wrapped\n\u001b[0;32m---> 62\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/numpy/numpy_array.py:161\u001b[0m, in \u001b[0;36m_concat\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    160\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 161\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43m_args_to_matching_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(values, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_probability/python/internal/backend/numpy/numpy_array.py:139\u001b[0m, in \u001b[0;36m_args_to_matching_arrays\u001b[0;34m(args_list, dtype_hint)\u001b[0m\n\u001b[1;32m    137\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[0;32m--> 139\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241m.\u001b[39mis_tensor(arg):\n\u001b[1;32m    140\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09953b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
