{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5LBh6_lOVBdN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <center> CS6700: Reinforcement Learning\n",
        "## <center> Gautham Govind A, EE19B022"
      ],
      "metadata": {
        "id": "34Huakf2BWGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tutorial 5 - Options Intro\n",
        "\n",
        "Please complete this tutorial to get an overview of options and an implementation of SMDP Q-Learning and Intra-Option Q-Learning.\n",
        "\n",
        "\n",
        "### References:\n",
        "\n",
        " [Recent Advances in Hierarchical Reinforcement\n",
        "Learning](https://people.cs.umass.edu/~mahadeva/papers/hrl.pdf) is a strong recommendation for topics in HRL that was covered in class. Watch Prof. Ravi's lectures on moodle or nptel for further understanding the core concepts. Contact the TAs for further resources if needed. \n"
      ],
      "metadata": {
        "id": "pn7PKu9r0asK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For compatibility with the given template, we use an older version of gym environment:"
      ],
      "metadata": {
        "id": "SUiZE656BQIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym==0.15.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60qCt1Yij6nw",
        "outputId": "cab31cba-8b46-4c7c-b842-f9f43af82fe1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.15.3 in /usr/local/lib/python3.9/dist-packages (0.15.3)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.15.3) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from gym==0.15.3) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from gym==0.15.3) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.15.3) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gym==0.15.3) (1.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.3) (0.18.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "A bunch of imports, you don't have to worry about these\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "#from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "P_DODRgW_ZKS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The environment used here is extremely similar to the openai gym ones.\n",
        "At first glance it might look slightly different. \n",
        "The usual commands we use for our experiments are added to this cell to aid you\n",
        "work using this environment.\n",
        "'''\n",
        "\n",
        "#Setting up the environment\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
        "env = CliffWalkingEnv()\n",
        "\n",
        "env.reset()\n",
        "\n",
        "#Current State\n",
        "print(env.s)\n",
        "\n",
        "# 4x12 grid = 48 states\n",
        "print (\"Number of states:\", env.nS)\n",
        "\n",
        "# Primitive Actions\n",
        "action = [\"up\", \"right\", \"down\", \"left\"]\n",
        "#correspond to [0,1,2,3] that's actually passed to the environment\n",
        "\n",
        "# either go left, up, down or right\n",
        "print (\"Number of actions that an agent can take:\", env.nA)\n",
        "\n",
        "# Example Transitions\n",
        "rnd_action = random.randint(0, 3)\n",
        "print (\"Action taken:\", action[rnd_action])\n",
        "next_state, reward, is_terminal, t_prob = env.step(rnd_action)\n",
        "print (\"Transition probability:\", t_prob)\n",
        "print (\"Next state:\", next_state)\n",
        "print (\"Reward recieved:\", reward)\n",
        "print (\"Terminal state:\", is_terminal)\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNA5kiH_esJ",
        "outputId": "f3f16370-97f5-4688-9203-a778c5590802"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n",
            "Number of states: 48\n",
            "Number of actions that an agent can take: 4\n",
            "Action taken: up\n",
            "Transition probability: {'prob': 1.0}\n",
            "Next state: 24\n",
            "Reward recieved: -1\n",
            "Terminal state: False\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Options\n",
        "We custom define very simple options here. They might not be the logical options for this settings deliberately chosen to visualise the Q Table better.\n"
      ],
      "metadata": {
        "id": "apuaOxavDXus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> We slightly modify the termination condition for Close option. </b> This is because if the agent attempts to do this option from the start state (state number 36), int(state/12) = 3, and therefore the termination condition is never reached, <b>forcing the agent to enter an endelss loop </b>. We circumvent this issue by modifying the condition from (int(state/12) == 2) to (int(state/12) >= 2), thereby breaking loop at the starting state as well."
      ],
      "metadata": {
        "id": "Z-iyLGSYBzTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are defining two more options here\n",
        "# Option 1 [\"Away\"] - > Away from Cliff (ie keep going up)\n",
        "# Option 2 [\"Close\"] - > Close to Cliff (ie keep going down) \n",
        "\n",
        "def Away(env,state):\n",
        "    \n",
        "    optdone = False\n",
        "    optact = 0\n",
        "    \n",
        "    if (int(state/12) == 0):\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "    \n",
        "\n",
        "def Close(env,state):\n",
        "    \n",
        "    optdone = False\n",
        "    optact = 2\n",
        "    \n",
        "    # We slightly mpodify the termination condition for Close option to break loop at start state\n",
        "    if (int(state/12) >= 2):\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "    \n",
        "    \n",
        "print('''\n",
        "Now the new action space will contain\n",
        "Primitive Actions: [\"up\", \"right\", \"down\", \"left\"]\n",
        "Options: [\"Away\",\"Close\"]\n",
        "Total Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\n",
        "Corresponding to [0,1,2,3,4,5]\n",
        "''')"
      ],
      "metadata": {
        "id": "g4MRC1p2DZbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7701e775-4316-4a93-9736-48d144d30a05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Now the new action space will contain\n",
            "Primitive Actions: [\"up\", \"right\", \"down\", \"left\"]\n",
            "Options: [\"Away\",\"Close\"]\n",
            "Total Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\n",
            "Corresponding to [0,1,2,3,4,5]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "Complete the code cell below\n"
      ],
      "metadata": {
        "id": "Jmv5c0XoK8GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q-Table: (States x Actions) === (env.ns(48) x total actions(6))\n",
        "\n",
        "\n",
        "# Q-Tables for SMDP and intra-option q-learning\n",
        "\n",
        "q_values_SMDP = np.zeros((48,6))\n",
        "q_values_IO = np.zeros((48,6))\n",
        "\n",
        "# Update_Frequency tables for SMDP and intra-option q-learning\n",
        "\n",
        "update_freq_SMDP = np.zeros((48,6))\n",
        "update_freq_IO = np.zeros((48,6))\n",
        "\n",
        "# Epsilon-greedy action selection function\n",
        "def egreedy_policy(q_values, state, epsilon):\n",
        "  if ( (np.random.rand() < epsilon) or (not q_values[state, :].any()) ): \n",
        "        return np.random.choice(np.arange(0, 6))\n",
        "  else:\n",
        "        return np.argmax(q_values[state, :])"
      ],
      "metadata": {
        "id": "bh_oghc7Ledh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "Below is an incomplete code cell with the flow of SMDP Q-Learning. Complete the cell and train the agent using SMDP Q-Learning algorithm.\n",
        "Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)"
      ],
      "metadata": {
        "id": "N8VJYkqoLqlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### SMDP Q-Learning \n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.9\n",
        "alpha = 0.1\n",
        "\n",
        "# Iterate over 1000 episodes\n",
        "for _ in range(1000):\n",
        "\n",
        "  state = env.reset()   \n",
        "  done = False\n",
        "\n",
        "  # Loop till environment is solved\n",
        "  while not done:\n",
        "    \n",
        "    # Epsilon-greedy action selection\n",
        "    action = egreedy_policy(q_values_SMDP, state, epsilon=0.1)\n",
        "      \n",
        "    # Primitive actions\n",
        "    if action < 4:\n",
        "            \n",
        "      # Regular Q-Learning update\n",
        "      next_state, reward, done, dummy = env.step(action)\n",
        "      q_values_SMDP[state, action] +=  alpha*(reward + gamma*np.max(q_values_SMDP[next_state, :4]) - q_values_SMDP[state, action])\n",
        "      update_freq_SMDP[state, action] += 1\n",
        "      state = next_state\n",
        "  \n",
        "    # Away option\n",
        "    if action == 4: \n",
        "\n",
        "      # Initialize reward bar, start_state and optdone\n",
        "      reward_bar = 0             \n",
        "      optdone = False\n",
        "      start_state = state\n",
        "      time_steps = 0\n",
        "     \n",
        "      # To account for the case where agent is already at the top boundary, we add an environment step outside the loop\n",
        "      # Otherwise, away option, which leads to an exit from the gridworld will yield 0 reward, thereby making this favourbale\n",
        "      # This will lead to undesirable behaviour and so we add this block outside the loop\n",
        "      optact,_ = Away(env,state)\n",
        "      next_state, reward, done,_ = env.step(optact)\n",
        "      time_steps += 1     \n",
        "      reward_bar +=  (gamma**(time_steps - 1))*reward\n",
        "      state = next_state\n",
        "\n",
        "      while (optdone == False):\n",
        "\n",
        "        # Apply the option\n",
        "        optact,optdone = Away(env,state)\n",
        "\n",
        "        # If option is done, update and terminate\n",
        "        # We multiply with gamma^time_steps to allow for correct scaling\n",
        "        if optdone:\n",
        "          q_values_SMDP[start_state, action] += alpha*(reward_bar + (gamma**time_steps)*np.max(q_values_SMDP[state, 4:]) - q_values_SMDP[start_state, action])\n",
        "          update_freq_SMDP[start_state, action] += 1\n",
        "          break\n",
        "\n",
        "        # Note that we also change the reward_bar update slightly\n",
        "        # This is because we apply discounting to the later rewards and not the initial rewards\n",
        "        # We keep track of how late a reward is using the time_steps variable\n",
        "        next_state, reward, done,_ = env.step(optact)\n",
        "        time_steps += 1      \n",
        "        reward_bar +=  (gamma**(time_steps - 1))*reward      \n",
        "        state = next_state\n",
        "     \n",
        "    # Close option\n",
        "    # Except for using close option, everything is identical to the code for away option\n",
        "    if action == 5: \n",
        "\n",
        "      #print(\"Close:\")\n",
        "      reward_bar = 0 \n",
        "      optdone = False\n",
        "      start_state = state\n",
        "      time_steps = 0\n",
        "\n",
        "      optact,_ = Close(env,state)\n",
        "      next_state, reward, done,_ = env.step(optact)\n",
        "      time_steps += 1     \n",
        "      reward_bar +=  (gamma**(time_steps - 1))*reward\n",
        "      state = next_state\n",
        "\n",
        "      while (optdone == False):\n",
        "                \n",
        "        optact,optdone = Close(env,state)\n",
        "\n",
        "        if optdone:\n",
        "          q_values_SMDP[start_state, action] +=  alpha*(reward_bar + (gamma**time_steps)*np.max(q_values_SMDP[state, 4:]) - q_values_SMDP[start_state, action])\n",
        "          update_freq_SMDP[start_state, action] += 1\n",
        "          break\n",
        "\n",
        "        next_state, reward, done,_ = env.step(optact)\n",
        "        time_steps += 1     \n",
        "        reward_bar +=  (gamma**(time_steps - 1))*reward\n",
        "        state = next_state\n"
      ],
      "metadata": {
        "id": "ok_5eQM7OCTj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "Using the same options and the SMDP code, implement Intra Option Q-Learning (In the code cell below). You *might not* always have to search through options to find the options with similar policies, think about it. Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)\n",
        "\n"
      ],
      "metadata": {
        "id": "3SQFbRCHWQyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our particular case, we only have two options: Away and Close.\n",
        "Away always generates an UP action, whereas Close always generates a DOWN action.\n",
        "It is therefore evident that these two options can <b> never have the same action choice </b>. Therefore, for this particular problem, it is sufficient to update the two options separately as they have no similar policies."
      ],
      "metadata": {
        "id": "40dAGJnz8SJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLQlUha39QdW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Intra Option Q-Learning \n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.9\n",
        "alpha = 0.1\n",
        "\n",
        "# Iterate over 1000 episodes\n",
        "for _ in range(1000):\n",
        "\n",
        "  state = env.reset()    \n",
        "  done = False\n",
        "\n",
        "\n",
        "  while not done:\n",
        "                   \n",
        "    # Epsilon-greedy action selection\n",
        "    action = egreedy_policy(q_values_IO, state, epsilon=0.1)\n",
        "      \n",
        "    # Primitive action selection; identical to SMDP\n",
        "    if action < 4:\n",
        "            \n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      q_values_IO[state, action] +=  alpha*(reward + gamma*np.max(q_values_IO[next_state, :4]) - q_values_IO[state, action])\n",
        "      update_freq_IO[state, action] += 1\n",
        "      state = next_state\n",
        "  \n",
        "    # Away option\n",
        "    if action == 4:\n",
        "\n",
        "      # Initializing variables            \n",
        "      optdone = False\n",
        "      start_state = state\n",
        "    \n",
        "      # As with SMDP case, we add an environment step outside the loop\n",
        "      \n",
        "\n",
        "      optact,_ = Away(env,state)\n",
        "      next_state, reward, done,_ = env.step(optact)\n",
        "      state = next_state\n",
        "\n",
        "      while (optdone == False):\n",
        "\n",
        "        optact,optdone = Away(env,state)\n",
        "\n",
        "        # If option is done, use the termination update condition and break\n",
        "        if optdone:\n",
        "          q_values_IO[start_state, action] += alpha*(reward + (gamma)*np.max(q_values_IO[state, :4]) - q_values_IO[start_state, action])\n",
        "          update_freq_IO[start_state, action] += 1\n",
        "          break\n",
        "        \n",
        "        # For other cases, use the following update on a loop\n",
        "        q_values_IO[start_state, action] += alpha*(reward + (gamma)*np.max(q_values_IO[state, action]) - q_values_IO[start_state, action])\n",
        "        update_freq_IO[start_state, action] += 1\n",
        "\n",
        "        # Note also that in this case reward_bar and time_steps are unnecessary as we update on every step\n",
        "\n",
        "        next_state, reward, done,_ = env.step(optact)  \n",
        "        start_state = state   \n",
        "        state = next_state\n",
        "     \n",
        "    # Close option\n",
        "    # Again identical to away except for the fact that close option is used\n",
        "    if action == 5: \n",
        "      \n",
        "      optdone = False\n",
        "      start_state = state\n",
        "\n",
        "      optact,_ = Close(env,state)\n",
        "      next_state, reward, done,_ = env.step(optact)\n",
        "      state = next_state\n",
        "\n",
        "      while (optdone == False):\n",
        "                \n",
        "        optact,optdone = Close(env,state)\n",
        "\n",
        "        if optdone:\n",
        "          q_values_IO[start_state, action] += alpha*(reward + (gamma)*np.max(q_values_IO[state, :4]) - q_values_IO[start_state, action])\n",
        "          update_freq_IO[start_state, action] += 1\n",
        "          break\n",
        "        \n",
        "        q_values_IO[start_state, action] += alpha*(reward + (gamma)*np.max(q_values_IO[state, action]) - q_values_IO[start_state, action])\n",
        "        update_freq_IO[start_state, action] += 1\n",
        "\n",
        "        next_state, reward, done,_ = env.step(optact)  \n",
        "        start_state = state   \n",
        "        state = next_state\n"
      ],
      "metadata": {
        "id": "r6A2TdUHWVUN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4\n",
        "Compare the two Q-Tables and Update Frequencies and provide comments."
      ],
      "metadata": {
        "id": "JzUgcwL-VfkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first take argmax over all actions/ options for each state for SMDP and intra option q-learning. This gives us an idea of the policy that will be learnt."
      ],
      "metadata": {
        "id": "qYwGe30ZJCuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following visualization, the print statement is structured such that each action corresponds to the optimal action at that particular grid cell, i.e., the action at grid position (3, 0) would correspond to the action at the start state."
      ],
      "metadata": {
        "id": "IMdeoCviNs6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q-Table for SMDP Q-Learning:\")\n",
        "print(np.argmax(q_values_SMDP, axis = 1).reshape(4, 12))"
      ],
      "metadata": {
        "id": "v8mZE74_Vhmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b241b2-824b-4d13-c0fe-a1106ac907fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table for SMDP Q-Learning:\n",
            "[[2 3 0 4 2 1 1 1 1 1 2 5]\n",
            " [1 1 1 2 1 1 4 1 2 2 1 2]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q-Table for Intra-Option Q-Learning:\")\n",
        "print(np.argmax(q_values_IO, axis = 1).reshape(4, 12))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5rxiZ_8_yKa",
        "outputId": "d9120c99-1362-4cc2-ab70-b119594528e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table for Intra-Option Q-Learning:\n",
            "[[1 5 1 1 2 1 3 1 4 5 4 2]\n",
            " [1 1 1 1 1 2 1 1 1 1 5 2]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We make the following observations from the Q-value table:\n",
        "- As expected, both methods learn to select the correct action near the start state. We can see that last two rows are identical for both methods.\n",
        "- Being greedy with respect to the q-function from the start state leads to the optimal policy for both methods.\n",
        "- Both methods deviate from the optimal policy slightly in the first two rows. This could be because the start state is in the last row and hence these rows may not be explore enough. Note however that this does not lead to suboptimal results as we are only interested in a fixed start state."
      ],
      "metadata": {
        "id": "24l49zXcJzB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(update_freq_SMDP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XPpH3zzd7UE",
        "outputId": "9c833b65-929f-4d3e-c902-b1490d6cd645"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 129.  215.  184.  129.  129.   61.]\n",
            " [ 122.  235.  175.   92.  123.   60.]\n",
            " [ 114.  235.  169.   85.  114.   57.]\n",
            " [ 106.  231.  157.   78.  105.   54.]\n",
            " [  98.  222.  147.   71.   97.   51.]\n",
            " [  89.  209.  137.   65.   89.   46.]\n",
            " [  80.  198.  124.   58.   79.   42.]\n",
            " [  70.  182.  111.   51.   70.   37.]\n",
            " [  61.  154.  100.   44.   61.   34.]\n",
            " [  52.  129.   90.   37.   51.   28.]\n",
            " [  42.   91.   80.   31.   42.   26.]\n",
            " [  32.   32.   76.   25.   32.   48.]\n",
            " [  97.  211.   84.  124.   95.   81.]\n",
            " [  89.  227.   88.   88.   90.   81.]\n",
            " [  83.  233.   85.   79.   82.   77.]\n",
            " [  76.  233.   81.   73.   75.   72.]\n",
            " [  69.  217.   76.   68.   69.   67.]\n",
            " [  62.  205.   72.   60.   60.   60.]\n",
            " [  55.  190.   68.   53.   53.   55.]\n",
            " [  48.  175.   66.   45.   46.   47.]\n",
            " [  40.  151.   63.   40.   40.   41.]\n",
            " [  34.  130.   63.   32.   33.   33.]\n",
            " [  24.   97.   66.   24.   26.   31.]\n",
            " [  18.   21.   74.   18.   18.   74.]\n",
            " [ 296. 1255.   94.  143.   88.  101.]\n",
            " [ 198. 1139.   26.   94.   83.   36.]\n",
            " [ 170. 1072.   23.   92.   68.   18.]\n",
            " [ 152. 1012.   21.   85.   63.   27.]\n",
            " [ 126.  957.   25.   81.   57.   22.]\n",
            " [ 112.  934.   15.   62.   55.   16.]\n",
            " [ 100.  897.   18.   59.   54.   26.]\n",
            " [  86.  871.   18.   55.   45.   19.]\n",
            " [  68.  868.   13.   47.   33.   18.]\n",
            " [  60.  864.   16.   38.   32.   15.]\n",
            " [  40.  886.   15.   33.   24.   20.]\n",
            " [  29.   26.  661.   31.   22.  339.]\n",
            " [1514.   32.  160.  155.   88.  150.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(update_freq_IO)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZgXe-6edL0v",
        "outputId": "c9d0800c-bed4-4b40-969b-13fdfa315cbc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 128.  214.  135.  128.  127.  105.]\n",
            " [ 120.  229.  125.   91.  119.  108.]\n",
            " [ 112.  233.  118.   82.  111.  105.]\n",
            " [ 104.  224.  111.   75.  103.   99.]\n",
            " [  95.  221.  102.   68.   94.   92.]\n",
            " [  86.  202.   93.   66.   85.   87.]\n",
            " [  77.  181.   83.   55.   76.   77.]\n",
            " [  67.  159.   75.   49.   67.   68.]\n",
            " [  58.  136.   67.   42.   57.   62.]\n",
            " [  50.  108.   60.   36.   49.   54.]\n",
            " [  40.   72.   53.   30.   39.   51.]\n",
            " [  31.   31.   52.   26.   31.   51.]\n",
            " [ 120.  203.   95.  125.  243.  122.]\n",
            " [ 114.  230.  106.   90.  143.  125.]\n",
            " [ 106.  232.  105.   80.  128.  120.]\n",
            " [  96.  228.  100.   73.  112.  115.]\n",
            " [  87.  220.   94.   66.  106.  105.]\n",
            " [  77.  206.   87.   60.   95.   98.]\n",
            " [  68.  189.   82.   52.   77.   87.]\n",
            " [  58.  175.   75.   46.   70.   82.]\n",
            " [  48.  148.   69.   40.   64.   76.]\n",
            " [  38.  121.   65.   31.   54.   69.]\n",
            " [  30.   83.   64.   25.   42.   65.]\n",
            " [  19.   23.   81.   19.   38.   83.]\n",
            " [ 221. 1360.  107.  147.  232.  100.]\n",
            " [ 157. 1231.   35.   95.  135.   34.]\n",
            " [ 129. 1149.   33.   96.  116.   24.]\n",
            " [ 117. 1082.   26.   91.   99.   25.]\n",
            " [  98. 1027.   24.   76.   98.   23.]\n",
            " [  90.  983.   22.   65.   81.   31.]\n",
            " [  78.  951.   27.   60.   65.   24.]\n",
            " [  72.  930.   25.   53.   62.   18.]\n",
            " [  56.  900.   26.   52.   54.   24.]\n",
            " [  51.  892.   18.   37.   40.   25.]\n",
            " [  34.  920.   21.   29.   31.   16.]\n",
            " [  26.   34.  660.   30.   28.  340.]\n",
            " [1510.   37.  156.  158.  198.  164.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.    0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We make the following observations from the update_frequency table:\n",
        "- The last 11 rows are not updated for either methods. This is to be expected because these correspond to the cliff and the terminal states. Since these are not valid states (as per gym documentation), it makes sense that updates do not happen.\n",
        "- The primitive actions have similar update frequency for both methods. This is to be expected as both have the same code for updating q-value of primitive actions.\n",
        "- We see that the update frequency is much higher for options in intra-option q-learning as compared to SMDP. This is because in intra-option q-learning, we update the q-value on every step of the option, whereas in SMDP we only make one update once the option terminates. Of course this is in agreement with the general theme of intra-option q-learning as it was originally designed keeping in mind the low update-frequency of SMDP q-learning.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SemE13ORV04n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elt_te1fMd84"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}