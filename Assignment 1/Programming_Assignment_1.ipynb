{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkFS242HMrS3/y5IemIUu7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blaze010/CS6700-Reinforcement_Learning/blob/main/Assignment%201/Programming_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center >CS6700: Reinforcement Learning\n",
        "## <center >Programming Assignment 1\n",
        "## <center> TD Learning: SARSA and Q-Learning\n",
        "###  Submitted by: \n",
        "### Gautham Govind A: EE19B022\n",
        "### Vishnu Vinod: CS19B048 "
      ],
      "metadata": {
        "id": "1JiAc34_Ixg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary packages"
      ],
      "metadata": {
        "id": "QIdfqnCWJV5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import floor\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Pqvk6fcRJvnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "92G2tnyyKIWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We require functions capable of converting from row-column based indexing and sequential indexing of grid cells. These are defined below: "
      ],
      "metadata": {
        "id": "eTJ_Hxc2UxYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts row_column format to sequential (state number) format\n",
        "# Input  - 2D array of grid cells in (row, col) format\n",
        "# Output - 1D array of corresponding state numbers\n",
        "def row_col_to_seq(row_col, num_cols): \n",
        "    return row_col[:,0] * num_cols + row_col[:,1]\n",
        "\n",
        "# Converts sequential (state number) format to row_column format \n",
        "# Input  - 1D array of grid cells in state number format\n",
        "# Output - 2D array of grid cells in corresponding (row, col) format \n",
        "def seq_to_col_row(seq, num_cols): \n",
        "    r = floor(seq / num_cols)\n",
        "    c = seq - r * num_cols\n",
        "    return np.array([[r, c]])"
      ],
      "metadata": {
        "id": "i3BOKyqxVAer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the environment"
      ],
      "metadata": {
        "id": "SnJXDjixV8pp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment class, the definition of which has already been provided as part of the problem statement, is defined here:"
      ],
      "metadata": {
        "id": "kXwJVRiaWA8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorld:\n",
        "\n",
        "    \"\"\"\n",
        "    Creates a gridworld object to pass to an RL algorithm.\n",
        "    Parameters:\n",
        "    ----------\n",
        "    num_rows : int\n",
        "        The number of rows in the gridworld.\n",
        "    num_cols : int\n",
        "        The number of cols in the gridworld.\n",
        "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
        "        The start state of the gridworld (can only be one start state)\n",
        "    goal_states : numpy arrany of shape (n, 2)\n",
        "        The goal states for the gridworld where n is the number of goal\n",
        "        states.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
        "        self.num_rows = num_rows\n",
        "        self.num_cols = num_cols\n",
        "        self.start_state = start_state\n",
        "        self.goal_states = goal_states\n",
        "        self.obs_states = None\n",
        "        self.bad_states = None\n",
        "        self.num_bad_states = 0\n",
        "        self.p_good_trans = None\n",
        "        self.bias = None\n",
        "        self.r_step = None\n",
        "        self.r_goal = None\n",
        "        self.r_dead = None\n",
        "        self.gamma = 1 # default is no discounting\n",
        "        self.wind = wind\n",
        "\n",
        "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
        "\n",
        "        self.obs_states = obstructed_states\n",
        "        self.bad_states = bad_states\n",
        "        if bad_states is not None:\n",
        "            self.num_bad_states = bad_states.shape[0]\n",
        "        else:\n",
        "            self.num_bad_states = 0\n",
        "        self.restart_states = restart_states\n",
        "        if restart_states is not None:\n",
        "            self.num_restart_states = restart_states.shape[0]\n",
        "        else:\n",
        "            self.num_restart_states = 0\n",
        "\n",
        "    def add_transition_probability(self, p_good_transition, bias):\n",
        "\n",
        "        self.p_good_trans = p_good_transition\n",
        "        self.bias = bias\n",
        "\n",
        "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
        "\n",
        "        self.r_step = step_reward\n",
        "        self.r_goal = goal_reward\n",
        "        self.r_bad = bad_state_reward\n",
        "        self.r_restart = restart_state_reward\n",
        "\n",
        "\n",
        "    def create_gridworld(self):\n",
        "\n",
        "        self.num_actions = 4\n",
        "        self.num_states = self.num_cols * self.num_rows# +1\n",
        "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
        "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
        "\n",
        "        # rewards structure\n",
        "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
        "        #self.R[self.num_states-1] = 0\n",
        "        self.R[self.goal_states_seq] = self.r_goal\n",
        "        \n",
        "        for i in range(self.num_bad_states):\n",
        "            if self.r_bad is None:\n",
        "                raise Exception(\"Bad state specified but no reward is given\")\n",
        "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
        "            #print(\"bad states\", bad_state)\n",
        "            self.R[bad_state, :] = self.r_bad\n",
        "        for i in range(self.num_restart_states):\n",
        "            if self.r_restart is None:\n",
        "                raise Exception(\"Restart state specified but no reward is given\")\n",
        "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
        "            #print(\"restart_state\", restart_state)\n",
        "            self.R[restart_state, :] = self.r_restart\n",
        "\n",
        "        # probability model\n",
        "        if self.p_good_trans == None:\n",
        "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
        "\n",
        "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
        "        for action in range(self.num_actions):\n",
        "            for state in range(self.num_states):\n",
        "\n",
        "\n",
        "                # check if the state is the goal state or an obstructed state - transition to end\n",
        "                row_col = seq_to_col_row(state, self.num_cols)\n",
        "                if self.obs_states is not None:\n",
        "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
        "                else:\n",
        "                    end_states = self.goal_states\n",
        "\n",
        "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
        "                    self.P[state, state, action] = 1\n",
        "\n",
        "                # else consider stochastic effects of action\n",
        "                else:\n",
        "                    for dir in range(-1,2,1):\n",
        "                        \n",
        "                        direction = self._get_direction(action, dir)\n",
        "                        next_state = self._get_state(state, direction)\n",
        "                        if dir == 0:\n",
        "                            prob = self.p_good_trans\n",
        "                        elif dir == -1:\n",
        "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
        "                        elif dir == 1:\n",
        "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
        "\n",
        "                        self.P[state, next_state, action] += prob\n",
        "\n",
        "                # make restart states transition back to the start state with\n",
        "                # probability 1\n",
        "                if self.restart_states is not None:\n",
        "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
        "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
        "                        self.P[state,:,:] = 0\n",
        "                        self.P[state,next_state,:] = 1\n",
        "        return self\n",
        "\n",
        "    def _get_direction(self, action, direction):\n",
        "\n",
        "        left = [2,3,1,0]\n",
        "        right = [3,2,0,1]\n",
        "        if direction == 0:\n",
        "            new_direction = action\n",
        "        elif direction == -1:\n",
        "            new_direction = left[action]\n",
        "        elif direction == 1:\n",
        "            new_direction = right[action]\n",
        "        else:\n",
        "            raise Exception(\"getDir received an unspecified case\")\n",
        "        return new_direction\n",
        "\n",
        "    def _get_state(self, state, direction):\n",
        "\n",
        "        row_change = [-1,1,0,0]\n",
        "        col_change = [0,0,-1,1]\n",
        "        row_col = seq_to_col_row(state, self.num_cols)\n",
        "        row_col[0,0] += row_change[direction]\n",
        "        row_col[0,1] += col_change[direction]\n",
        "\n",
        "        # check for invalid states\n",
        "        if self.obs_states is not None:\n",
        "            if (np.any(row_col < 0) or\n",
        "                np.any(row_col[:,0] > self.num_rows-1) or\n",
        "                np.any(row_col[:,1] > self.num_cols-1) or\n",
        "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
        "                next_state = state\n",
        "            else:\n",
        "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
        "        else:\n",
        "            if (np.any(row_col < 0) or\n",
        "                np.any(row_col[:,0] > self.num_rows-1) or\n",
        "                np.any(row_col[:,1] > self.num_cols-1)):\n",
        "                next_state = state\n",
        "            else:\n",
        "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
        "\n",
        "        return next_state\n",
        "\n",
        "    def reset(self):\n",
        "      return int(self.start_state_seq)\n",
        "      \n",
        "    def step(self, state, action):\n",
        "        p, r = 0, np.random.random()\n",
        "        for next_state in range(self.num_states):\n",
        "            \n",
        "            p += self.P[state, next_state, action]\n",
        "            \n",
        "            if r <= p:\n",
        "                break\n",
        "\n",
        "        if(self.wind and np.random.random() < 0.4):\n",
        "\n",
        "          arr = self.P[next_state, :, 3]\n",
        "          next_next = np.where(arr == np.amax(arr))\n",
        "          next_next = next_next[0][0]\n",
        "          return next_next, self.R[next_next]\n",
        "        else:\n",
        "          return next_state, self.R[next_state]"
      ],
      "metadata": {
        "id": "uzwVett_WwPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Definitions"
      ],
      "metadata": {
        "id": "Mbej1hYjaKOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we create an abstract BasePolicy() class:"
      ],
      "metadata": {
        "id": "y_B7nqBPaRZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasePolicy:\n",
        "\n",
        "  def select_action(self, state_id):\n",
        "    raise NotImplemented"
      ],
      "metadata": {
        "id": "E8wqIujOaWt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first create an epsilon greedy policy which makes use of action value functions as the quantity based on which actions are chosen:"
      ],
      "metadata": {
        "id": "qk5pElngdAdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedy(BasePolicy):\n",
        "\n",
        "# epsilon      : epsilon value to be used by epsilon-greedy\n",
        "# actions      : Possible actions that can be taken up in each state\n",
        "#              : Default set for the current problem \n",
        "  def __init__(self, epsilon, actions = np.array([0, 1, 2, 3])):\n",
        "    self.eps = epsilon\n",
        "    self.actions = actions\n",
        "\n",
        "# action_values: 2D array containing predicted action values for each (state, action) pair\n",
        "#              : array size is |S|*4, i.e, total number of states x total number of actions\n",
        "# state_id     : state for which action is to be determined\n",
        "  def select_action(self, state_id, action_values):\n",
        "\n",
        "    # To explore or not to explore is the question\n",
        "    explore_or_exploit = np.random.binomial(1, 1 - self.eps)\n",
        "\n",
        "    # if exploit, select the arm with maximum value of action value function\n",
        "    if explore_or_exploit == 1:\n",
        "        return self.actions[np.argmax(action_values[state_id, :])]\n",
        "\n",
        "    # if explore, choose any one of the arms randomly\n",
        "    else:\n",
        "        return np.random.choice(self.actions)\n"
      ],
      "metadata": {
        "id": "Tq9xPdMfdPBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftMax(BasePolicy):\n",
        "\n",
        "# beta      : temperature to be used by softmax function\n",
        "  def __init__(self, beta, actions = np.array([0, 1, 2, 3])):\n",
        "    self.beta = beta\n",
        "    self.actions = actions\n",
        "\n",
        "# action_values: 2D array containing predicted action values for each (state, action) pair\n",
        "#              : array size is |S|*4, i.e, total number of states x total number of actions\n",
        "# state_id     : state for which action is to be determined\n",
        "  def select_action(self, state_id, action_values):\n",
        "\n",
        "    # Finding the largest action value for this particular state\n",
        "    max_action_val = max(action_values[state_id, :])\n",
        "\n",
        "    # Computing the denominator of the softmax expression\n",
        "    # Note that each term is divided by the largest exponential\n",
        "    softmax_prob_denom = np.sum(np.exp((action_values[state_id, :] - max_action_val)/self.beta))\n",
        "\n",
        "    # Computing the numerator term for each action\n",
        "    # Again, terms are divided by the largest exponential to maintatin consistency with denominator\n",
        "    softmax_prob = np.array([ np.exp((x - max_action_val) /self.beta)/softmax_prob_denom for x in list(action_values[state_id, :])])\n",
        "\n",
        "    # sample according to the softmax distribution to get an arm\n",
        "    return np.random.choice(self.actions, p = softmax_prob)"
      ],
      "metadata": {
        "id": "ihnhSGZMi4P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA"
      ],
      "metadata": {
        "id": "EVbg8OyUl9ki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define the SARSA update rule:"
      ],
      "metadata": {
        "id": "vV-VfCfknCVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input:\n",
        "# q_current - Current value estimate for a particular (state,action) pair\n",
        "# q_future  - Current value estimate for the next (state, action) pair according to some policy \n",
        "# alpha     - Learning rate\n",
        "# gamma     - Discount factor\n",
        "\n",
        "# Output:\n",
        "# Modified value estimate for the current (state, action) pair\n",
        "\n",
        "def sarsa_update(q_current, q_future, reward, alpha, gamma):\n",
        "  return q_current + alpha*(reward + gamma*q_future - q_current)"
      ],
      "metadata": {
        "id": "XA4KvouYpPmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define an iteration procedure which essentially gives you the on-poilcy SARSA control algorithm:"
      ],
      "metadata": {
        "id": "W5mtmkcSpxs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 2\n",
        "ls = [2, 3, 4]\n",
        "print(x not in ls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bONUS1DrM7Xv",
        "outputId": "f3a1c459-2dae-459e-8d61-131228ee68f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(row_col_to_seq(np.array([[3, 6], [4, 5]]), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHJNMrSDNITL",
        "outputId": "0b0ef3a5-18dd-4835-8a11-edb98e117253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18, 21])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def iterator(env_instance, update_policy, exploration_policy, num_episodes, alpha, gamma):\n",
        "\n",
        "  # Arbitrary initialization\n",
        "  action_values = np.zeros((env_instance.num_states, env_instance.num_actions))\n",
        "\n",
        "  for episode in range(num_episodes):\n",
        "    current_state  = row_col_to_seq(env_instance.start_state, env_instance.num_cols)[0]\n",
        "    current_action = exploration_policy.select_action(current_state, action_values)\n",
        "\n",
        "    timestep = 0\n",
        "\n",
        "    while((current_state not in list(row_col_to_seq(env_instance.goal_states, env_instance.num_cols))) and (timestep < 100)):\n",
        "\n",
        "      if episode == (num_episodes-1):\n",
        "        print(seq_to_col_row(current_state, env_instance.num_cols), end = '-->')\n",
        "\n",
        "      next_state, reward = env_instance.step(current_state, current_action)\n",
        "      next_action = exploration_policy.select_action(next_state, action_values)\n",
        "      action_values[current_state, current_action] = update_policy(action_values[current_state, current_action], action_values[next_state, next_action], reward, alpha, gamma)\n",
        "      current_state = next_state\n",
        "      current_action = next_action\n",
        "      timestep += 1\n",
        "      \n",
        "    if episode == (num_episodes-1):\n",
        "        print(seq_to_col_row(current_state, env_instance.num_cols))\n",
        "      \n",
        "    \n",
        "\n",
        "    \n",
        "  return action_values"
      ],
      "metadata": {
        "id": "uUXFJ1HIJ6V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "wGG8xCMFOz9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify world parameters\n",
        "num_cols = 10\n",
        "num_rows = 10\n",
        "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
        "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
        "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
        "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
        "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
        "restart_states = np.array([[3,7],[8,2]])\n",
        "start_state = np.array([[0,4]])\n",
        "goal_states = np.array([[0,9],[2,2],[8,7]])\n",
        "\n",
        "# create model\n",
        "gw = GridWorld(num_rows=num_rows,\n",
        "               num_cols=num_cols,\n",
        "               start_state=start_state,\n",
        "               goal_states=goal_states, wind = False)\n",
        "gw.add_obstructions(obstructed_states=obstructions,\n",
        "                    bad_states=bad_states,\n",
        "                    restart_states=restart_states)\n",
        "gw.add_rewards(step_reward=-1,\n",
        "               goal_reward=10,\n",
        "               bad_state_reward=-20,\n",
        "               restart_state_reward=-100)\n",
        "gw.add_transition_probability(p_good_transition=1.0,\n",
        "                              bias=0.5)\n",
        "env = gw.create_gridworld()"
      ],
      "metadata": {
        "id": "3V10qGTVP3uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps_greedy = EpsilonGreedy(epsilon = 0.1)\n",
        "\n",
        "test_op = iterator(env, sarsa_update, eps_greedy, 5000, 0.4, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YZ4lJaEQmAm",
        "outputId": "67b6f995-0164-4edd-f6ff-7ab40bfa7ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 4]]-->[[0 4]]-->[[0 5]]-->[[0 4]]-->[[0 3]]-->[[0 2]]-->[[0 1]]-->[[0 0]]-->[[1 0]]-->[[2 0]]-->[[3 0]]-->[[2 0]]-->[[3 0]]-->[[4 0]]-->[[4 1]]-->[[4 2]]-->[[3 2]]-->[[2 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qmp08RDzWQA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_op"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5oveDgoS-_7",
        "outputId": "3b4e74be-f097-49b5-d590-b12d5140af4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ -3.77099609,  -3.77368164,  -4.03808594,  -4.0703125 ],\n",
              "       [ -3.96679688,  -4.0703125 ,  -4.23095703,  -4.54907227],\n",
              "       [ -4.30078125,  -4.71582031,  -4.57958984,  -4.51953125],\n",
              "       [ -4.58740234,  -4.73535156,  -4.67382812,  -4.76660156],\n",
              "       [ -5.43612671,  -4.80932617,  -5.08938599,  -4.75      ],\n",
              "       [ -4.90014648,  -4.63964844,  -4.95654297,  -4.68188477],\n",
              "       [ -4.61035156,  -4.76855469,  -4.58007812,  -4.71777344],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -3.31445312,  -3.58740234,  -3.41796875,  -3.89208984],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -4.63006592,  -4.64208984,  -5.04467773,  -4.98632812],\n",
              "       [ -4.94555664,  -4.94946289,  -4.57531738,  -4.50537109],\n",
              "       [ -4.57666016,  -4.82226562,  -4.484375  ,  -4.66503906],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -3.16601562,  -3.32324219,  -3.0859375 ,  -3.41015625],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -4.54394531,  -4.8503418 ,  -4.64160156,  -4.84033203],\n",
              "       [ -4.91845703,  -5.19750977,  -4.98730469,  -4.54833984],\n",
              "       [ -4.55322266, -26.15625   ,  -4.76757812,  -4.75195312],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -2.7890625 ,  -2.92089844,  -3.30859375,  -3.13476562],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  5.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -4.72167969,  -5.5       ,  -5.07592773,  -5.21459961],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -1.25      ,  -1.125     ,  -0.75      , -75.        ],\n",
              "       [ -0.875     ,  -1.453125  ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -2.6640625 ,  -2.703125  ,  -2.5546875 ,  -2.515625  ],\n",
              "       [ -2.25      ,  -2.375     ,  -2.65625   ,  -3.        ],\n",
              "       [ -0.5       ,  -0.5       ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -2.45117188,  -1.        ,  -4.75      ,  -3.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -1.125     ,  -1.        ,  -1.625     ,  -1.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -2.46875   ,  -2.4453125 ,  -2.46875   ,  -2.25      ],\n",
              "       [ -2.296875  ,  -2.28125   ,  -2.375     ,  -2.15625   ],\n",
              "       [ -3.        ,  -1.90625   ,  -2.296875  ,  -2.5       ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -3.        ,  -1.125     ,  -0.75      ,  -1.        ],\n",
              "       [ -0.75      ,  -0.75      ,  -1.125     ,  -0.5       ],\n",
              "       [ -1.125     ,  -0.75      ,  -1.        ,  -1.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -2.75      ,  -2.3671875 ,  -2.40625   ,  -2.28125   ],\n",
              "       [ -2.34375   ,  -2.3125    ,  -2.21875   , -14.25      ],\n",
              "       [ -1.6875    , -13.4375    ,  -2.03125   ,  -1.8125    ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -1.        ,  -0.75      ,  -0.75      ,  -0.5       ],\n",
              "       [ -0.5       ,  -4.5       ,  -0.75      ,  -0.5       ],\n",
              "       [ -1.        ,  -0.75      ,  -0.5       ,  -0.5       ],\n",
              "       [ -0.75      ,  -0.75      ,  -0.75      ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -2.73828125,  -2.6328125 ,  -2.5078125 ,  -2.97265625],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -1.25      , -75.        ,  -0.75      ,  -0.5       ],\n",
              "       [ -0.75      ,  -0.5       ,  -0.5       ,  -0.75      ],\n",
              "       [ -0.5       ,  -0.75      ,  -0.5       ,   0.        ],\n",
              "       [ -2.        ,  -4.5       ,  -0.5       ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -2.92578125,  -3.109375  ,  -3.1484375 ,  -3.37890625],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -1.94628906,  -2.19628906,   0.        ,   0.        ],\n",
              "       [ -0.5       ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -0.5       ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [ -3.18359375,  -3.0390625 ,  -2.75      ,  -3.42578125],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
              "       [  0.        ,   0.        ,   0.        ,   0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([[1,2, 3], [3, 10, 6], [5, 6, 7]])"
      ],
      "metadata": {
        "id": "OHwWMjLYRcYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wflGor-iSiVP",
        "outputId": "29c06b45-835e-40ca-e019-c5d0083bd5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [ 3, 10,  6],\n",
              "       [ 5,  6,  7]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(arr[1, :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1ZtaTIxSO3E",
        "outputId": "ca636bd0-a83d-4de2-d49f-94a01e992bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7MlDPR_zST0u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}