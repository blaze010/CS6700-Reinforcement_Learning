{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/blaze010/CS6700-Reinforcement_Learning/blob/main/Assignment%201/Programming_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JiAc34_Ixg9"
   },
   "source": [
    "# <center >CS6700: Reinforcement Learning\n",
    "## <center >Programming Assignment 1\n",
    "## <center> TD Learning: SARSA and Q-Learning\n",
    "###  Submitted by: \n",
    "### Gautham Govind A: EE19B022\n",
    "### Vishnu Vinod: CS19B048 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIdfqnCWJV5X"
   },
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pqvk6fcRJvnu"
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92G2tnyyKIWE"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTJ_Hxc2UxYR"
   },
   "source": [
    "We require functions capable of converting from row-column based indexing and sequential indexing of grid cells. These are defined below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i3BOKyqxVAer"
   },
   "outputs": [],
   "source": [
    "# Converts row_column format to sequential (state number) format\n",
    "# Input  - 2D array of grid cells in (row, col) format\n",
    "# Output - 1D array of corresponding state numbers\n",
    "def row_col_to_seq(row_col, num_cols): \n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "# Converts sequential (state number) format to row_column format \n",
    "# Input  - 1D array of grid cells in state number format\n",
    "# Output - 2D array of grid cells in corresponding (row, col) format \n",
    "def seq_to_col_row(seq, num_cols): \n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnJXDjixV8pp"
   },
   "source": [
    "## Defining the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXwJVRiaWA8P"
   },
   "source": [
    "The environment class, the definition of which has already been provided as part of the problem statement, is defined here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uzwVett_WwPW"
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "        \n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "                        \n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "        return int(self.start_state_seq)\n",
    "      \n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "            \n",
    "            p += self.P[state, next_state, action]\n",
    "            \n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "            arr = self.P[next_state, :, 3]\n",
    "            next_next = np.where(arr == np.amax(arr))\n",
    "            next_next = next_next[0][0]\n",
    "            return next_next, self.R[next_next]\n",
    "        else:\n",
    "            return next_state, self.R[next_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mbej1hYjaKOF"
   },
   "source": [
    "## Policy Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_B7nqBPaRZ5"
   },
   "source": [
    "First, we create an abstract BasePolicy() class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "E8wqIujOaWt3"
   },
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    def select_action(self, state_id):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk5pElngdAdg"
   },
   "source": [
    "We first create an epsilon greedy policy which makes use of action value functions as the quantity based on which actions are chosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Tq9xPdMfdPBJ"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy(BasePolicy):\n",
    "    # epsilon      : epsilon value to be used by epsilon-greedy\n",
    "    # actions      : Possible actions that can be taken up in each state\n",
    "    #              : Default set for the current problem\n",
    "    def __init__(self, epsilon, actions = np.array([0, 1, 2, 3])):\n",
    "        self.eps = epsilon\n",
    "        self.actions = actions\n",
    "\n",
    "    # action_values: 2D array containing predicted action values for each (state, action) pair\n",
    "    #              : array size is |S|*4, i.e, total number of states x total number of actions\n",
    "    # state_id     : state for which action is to be determined\n",
    "    def select_action(self, state_id, action_values):\n",
    "        # To explore or not to explore, that is the question\n",
    "        explore_or_exploit = np.random.binomial(1, 1 - self.eps)\n",
    "\n",
    "        # if exploit, select the arm with maximum value of action value function else choose random arm\n",
    "        if explore_or_exploit == 1:\n",
    "            return self.actions[np.argmax(action_values[state_id, :])]\n",
    "        else:\n",
    "            return np.random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ihnhSGZMi4P5"
   },
   "outputs": [],
   "source": [
    "class SoftMax(BasePolicy):\n",
    "\n",
    "    # beta      : temperature to be used by softmax function\n",
    "    def __init__(self, beta, actions = np.array([0, 1, 2, 3])):\n",
    "        self.beta = beta\n",
    "        self.actions = actions\n",
    "\n",
    "    # action_values: 2D array containing predicted action values for each (state, action) pair\n",
    "    #              : array size is |S|*4, i.e, total number of states x total number of actions\n",
    "    # state_id     : state for which action is to be determined\n",
    "    def select_action(self, state_id, action_values):\n",
    "\n",
    "        # Finding the largest action value for this particular state\n",
    "        max_action_val = max(action_values[state_id, :])\n",
    "\n",
    "        # Computing the denominator of the softmax expression\n",
    "        # Note that each term is divided by the largest exponential\n",
    "        softmax_prob_denom = np.sum(np.exp((action_values[state_id, :] - max_action_val)/self.beta))\n",
    "\n",
    "        # Computing the numerator term for each action\n",
    "        # Again, terms are divided by the largest exponential to maintatin consistency with denominator\n",
    "        softmax_prob = np.array([ np.exp((x - max_action_val) /self.beta)/softmax_prob_denom for x in list(action_values[state_id, :])])\n",
    "\n",
    "        # sample according to the softmax distribution to get an arm\n",
    "        return np.random.choice(self.actions, p = softmax_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVbg8OyUl9ki"
   },
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV-VfCfknCVt"
   },
   "source": [
    "We first define the SARSA update rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XA4KvouYpPmy"
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# q_current - Current estimate for a particular (state,action) pair\n",
    "# q_future - Future estimates for all actions on a particular state\n",
    "# reward - reward\n",
    "# next_action - specifies next action\n",
    "\n",
    "def sarsa_update(q_current, q_future, reward, alpha, gamma, next_action = None):\n",
    "    return q_current + alpha*(reward + gamma * q_future[next_action] - q_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the Q-Learning update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# q_current - Current estimate for a particular (state,action) pair\n",
    "# q_future - Future estimates for all actions on a particular state\n",
    "# reward - reward\n",
    "# next_action - specifies next action (redundant here)\n",
    "\n",
    "def qlearning_update(q_current, q_future, reward, alpha, gamma, next_action = None):\n",
    "    q_future_max = max(q_future)\n",
    "    return q_current + alpha*(reward + gamma * q_future_max - q_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterator for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingIterator:\n",
    "    def __init__(self, env, update_policy, exploration_policy, episodes, alpha, gamma):\n",
    "        self.env = env                         # Grid World Env\n",
    "        self.update = update_policy            # SARSA vs Q-Learning\n",
    "        self.explore = exploration_policy      # epsilon greedy vs softmax\n",
    "        self.num_eps = episodes                # training episodes\n",
    "        self.alpha = alpha                     # learning rate\n",
    "        self.gamma = gamma                     # discount factor\n",
    "        \n",
    "        self.steps = None\n",
    "        self.rewards = None\n",
    "        \n",
    "    def train(self):\n",
    "        # Arbitrary initialization\n",
    "        a_values = np.zeros((self.env.num_states, self.env.num_actions))\n",
    "\n",
    "        for episode in range(self.num_eps):\n",
    "\n",
    "            current_state  = row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "            current_action = self.explore.select_action(current_state, a_values)\n",
    "            timestep = 0\n",
    "\n",
    "            while((current_state not in list(row_col_to_seq(self.env.goal_states, self.env.num_cols))) and (timestep < 100)):\n",
    "\n",
    "                if episode == (self.num_eps-1):\n",
    "                    print(seq_to_col_row(current_state, self.env.num_cols), end = '-->')\n",
    "\n",
    "                next_state, reward = self.env.step(current_state, current_action)\n",
    "                next_action = self.explore.select_action(next_state, a_values)\n",
    "                a_values[current_state, current_action] = self.update(a_values[current_state, current_action], a_values[next_state], reward, self.alpha, self.gamma, next_action)\n",
    "                current_state = next_state\n",
    "                current_action = next_action\n",
    "                timestep += 1\n",
    "\n",
    "            if episode == (self.num_eps-1):\n",
    "                print(seq_to_col_row(current_state, self.env.num_cols))\n",
    "\n",
    "        return a_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify world parameters\n",
    "num_cols = 10\n",
    "num_rows = 10\n",
    "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "restart_states = np.array([[3,7],[8,2]])\n",
    "start_state = np.array([[0,4]])\n",
    "goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "# create model\n",
    "gw = GridWorld(num_rows=num_rows,\n",
    "               num_cols=num_cols,\n",
    "               start_state=start_state,\n",
    "               goal_states=goal_states, wind = False)\n",
    "\n",
    "gw.add_obstructions(obstructed_states=obstructions,\n",
    "                    bad_states=bad_states,\n",
    "                    restart_states=restart_states)\n",
    "\n",
    "gw.add_rewards(step_reward=-1,\n",
    "               goal_reward=10,\n",
    "               bad_state_reward=-20,\n",
    "               restart_state_reward=-100)\n",
    "\n",
    "gw.add_transition_probability(p_good_transition=1.0, bias=0.5)\n",
    "\n",
    "env = gw.create_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 4]]-->[[1 4]]-->[[2 4]]-->[[3 4]]-->[[4 4]]-->[[5 4]]-->[[6 4]]-->[[7 4]]-->[[8 4]]-->[[9 4]]-->[[9 5]]-->[[9 6]]-->[[9 6]]-->[[9 6]]-->[[9 6]]-->[[9 6]]-->[[9 6]]-->[[9 7]]-->[[8 7]]\n"
     ]
    }
   ],
   "source": [
    "eps_greedy = EpsilonGreedy(epsilon = 0.1)\n",
    "trainer = TrainingIterator(env, sarsa_update, eps_greedy, 5000, 0.4, 1)\n",
    "test_op = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.73892370e+01, -3.49748940e+01, -3.73347238e+01,\n",
       "        -3.71931799e+01],\n",
       "       [-3.83457605e+01, -3.83968720e+01, -3.73004781e+01,\n",
       "        -3.79349021e+01],\n",
       "       [-3.92592777e+01, -3.93210962e+01, -3.87703011e+01,\n",
       "        -3.94380993e+01],\n",
       "       [-3.94130155e+01, -3.91427018e+01, -3.96684345e+01,\n",
       "        -3.23973691e+01],\n",
       "       [-3.85570728e+01, -3.05414839e+01, -3.77780213e+01,\n",
       "        -3.99850081e+01],\n",
       "       [-3.97274072e+01, -3.97363064e+01, -3.95047840e+01,\n",
       "        -3.97721021e+01],\n",
       "       [-3.98824060e+01, -3.96379574e+01, -3.97690674e+01,\n",
       "        -3.97810698e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 7.73199360e+00,  2.14868589e+00,  8.95626030e+00,\n",
       "         1.00000000e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-3.65681189e+01, -3.14854498e+01, -3.64236875e+01,\n",
       "        -3.65027074e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-3.67727155e+01, -3.09939997e+01, -3.28355016e+01,\n",
       "        -3.80509965e+01],\n",
       "       [-3.94417695e+01, -3.76988301e+01, -3.78984186e+01,\n",
       "        -3.82265634e+01],\n",
       "       [-3.94867915e+01, -4.19606331e+01, -3.72713217e+01,\n",
       "        -3.98705158e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 8.99964529e+00,  1.68001082e+00,  6.11484617e+00,\n",
       "        -1.13378015e+01],\n",
       "       [ 9.93953382e+00,  1.99097279e+00,  4.86640892e+00,\n",
       "        -8.00000000e+00],\n",
       "       [-3.53813920e+01, -2.69803063e+01, -3.52980545e+01,\n",
       "        -3.53410524e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-3.70867159e+01, -3.15735556e+01, -3.74798828e+01,\n",
       "        -3.72671577e+01],\n",
       "       [-3.72162459e+01, -3.75055474e+01, -3.66913672e+01,\n",
       "        -3.71138428e+01],\n",
       "       [-3.68262135e+01, -5.25216354e+01, -3.68582270e+01,\n",
       "        -3.65238795e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 6.33880722e+00, -3.96163043e-01, -2.21235253e+00,\n",
       "        -2.12990032e+00],\n",
       "       [-1.17130240e+01, -2.15349715e+00,  5.59662917e+00,\n",
       "         1.60474307e-01],\n",
       "       [-3.46792208e+01, -2.48943113e+01, -3.44662585e+01,\n",
       "        -3.46116044e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 1.00000000e+01, -2.36846782e+01,  8.99999982e+00,\n",
       "         8.31000839e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-3.69492056e+01, -2.78524057e+01, -3.66556474e+01,\n",
       "        -3.69487915e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-1.85378154e+01, -7.28099584e+00, -1.19467261e+01,\n",
       "        -1.18993975e+02],\n",
       "       [-2.13698360e+01, -2.69600460e+01, -2.24230240e+01,\n",
       "        -2.01625252e+01],\n",
       "       [ 4.18927716e+00, -3.01059409e+00, -9.27205333e+01,\n",
       "        -2.89076551e+00],\n",
       "       [ 4.03886687e+00, -1.74538649e+00,  5.14474882e-01,\n",
       "        -3.04458215e+00],\n",
       "       [-3.38506162e+01, -3.37128589e+01, -3.40110368e+01,\n",
       "        -2.37544157e+01],\n",
       "       [-2.73800531e+01, -2.10378825e+01, -3.31313474e+01,\n",
       "        -3.42832289e+01],\n",
       "       [ 8.99999078e+00, -2.58247450e+01, -2.30031097e+01,\n",
       "        -1.67073281e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-3.33354205e+01, -4.45270163e+00, -2.48384260e+01,\n",
       "        -2.86285485e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-1.15249945e+01, -6.52451825e+00, -1.06260143e+01,\n",
       "        -9.44448136e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-7.73834518e+00, -5.88668799e+00, -1.99037669e+00,\n",
       "         3.01239135e+00],\n",
       "       [ 3.99379569e+00, -6.12910114e-01, -1.99074431e+00,\n",
       "         5.85565589e-01],\n",
       "       [-3.31071515e+01, -3.33304491e+01, -3.34899038e+01,\n",
       "        -3.32394018e+01],\n",
       "       [-3.29592347e+01, -3.28667876e+01, -3.29649663e+01,\n",
       "        -2.67712735e+01],\n",
       "       [-1.47381371e+01, -3.27041173e+01, -3.23696977e+01,\n",
       "        -2.69607337e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-3.57411302e+01, -3.27096583e+00, -7.72076228e+00,\n",
       "        -8.28001191e+00],\n",
       "       [-1.07158660e+01, -1.39183157e+01, -1.10518030e+01,\n",
       "        -6.90247623e+00],\n",
       "       [-1.11937618e+01, -5.59296887e+00, -1.04985068e+01,\n",
       "        -9.00701949e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 9.05553424e-01, -6.48292573e+00, -3.11620014e+00,\n",
       "        -3.01413536e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-3.30816613e+01, -3.29525602e+01, -3.33427604e+01,\n",
       "        -3.31781209e+01],\n",
       "       [-3.07274322e+01, -3.29404141e+01, -3.29544874e+01,\n",
       "        -3.31996375e+01],\n",
       "       [-3.39425268e+01, -6.87147208e+01, -3.32611154e+01,\n",
       "        -3.33923338e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-5.45222084e+00,  1.01315842e+00, -6.80579328e+00,\n",
       "        -7.08753305e+00],\n",
       "       [-1.05261013e+01, -1.79960185e+01, -1.05325547e+01,\n",
       "        -6.50822598e+00],\n",
       "       [-1.03075774e+01, -6.16540538e+00, -7.19778139e+00,\n",
       "        -4.44760827e+00],\n",
       "       [-7.56171100e+00, -9.64485185e+00, -8.08238108e+00,\n",
       "        -3.53582697e+00],\n",
       "       [-1.71268105e+00, -5.46131251e+00, -8.33425886e+00,\n",
       "        -5.09132957e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-3.29308658e+01, -3.26818311e+01, -3.26605080e+01,\n",
       "        -3.25633995e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-9.32609487e+00, -1.23872320e+02, -2.97135325e+01,\n",
       "        -1.05885974e+00],\n",
       "       [-2.32520229e+00, -1.89098481e+01, -3.99812223e+00,\n",
       "         3.26005171e+00],\n",
       "       [-1.37557808e+00,  4.41820898e+00, -7.27899729e+00,\n",
       "        -1.72697400e+01],\n",
       "       [-3.08669240e+00, -1.98392533e+01,  3.32421180e+00,\n",
       "        -1.15253251e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-3.26662582e+01, -3.25589043e+01, -3.26171385e+01,\n",
       "        -3.22673399e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-2.60957894e+01, -2.84034242e+01, -2.62537066e+01,\n",
       "        -3.05875287e+01],\n",
       "       [-4.56610686e-02,  2.48300422e+00, -1.01486216e+02,\n",
       "         1.90778314e+00],\n",
       "       [ 2.13742573e+00,  5.25362058e+00,  2.79302844e+00,\n",
       "         4.42287325e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-6.40000000e-01, -4.00000000e-01,  6.40000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-6.40000000e-01, -8.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-3.27084837e+01, -3.28810474e+01, -3.30193285e+01,\n",
       "        -3.25513994e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-4.00000000e+01, -2.66336256e+00, -2.38236160e+00,\n",
       "         3.61772099e+00],\n",
       "       [-3.60002459e-01,  3.76708673e+00,  1.89116075e+00,\n",
       "         5.47405365e+00],\n",
       "       [ 3.52466987e+00,  4.72896138e+00,  3.78961193e+00,\n",
       "         6.51683611e+00],\n",
       "       [ 6.35126557e+00,  6.44260857e+00,  4.34288793e+00,\n",
       "         7.27485639e+00],\n",
       "       [ 6.06382120e+00,  6.34954058e+00,  5.87409579e+00,\n",
       "         7.52968050e+00],\n",
       "       [ 1.00000000e+01,  7.63838650e+00,  6.84106830e+00,\n",
       "         5.58653027e+00],\n",
       "       [ 8.16000000e-01,  6.74373558e+00,  8.99912678e+00,\n",
       "        -8.00000000e+00],\n",
       "       [-4.00000000e-01, -1.28000000e+01,  3.16534069e+00,\n",
       "         0.00000000e+00]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_op"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNO5RZGk9oIMzG4WxkM2gWm",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
