{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/blaze010/CS6700-Reinforcement_Learning/blob/main/Assignment%201/Programming_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JiAc34_Ixg9"
   },
   "source": [
    "# <center >CS6700: Reinforcement Learning\n",
    "## <center >Programming Assignment 1\n",
    "## <center> TD Learning: SARSA and Q-Learning\n",
    "###  Submitted by: \n",
    "### Gautham Govind A: EE19B022\n",
    "### Vishnu Vinod: CS19B048 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIdfqnCWJV5X"
   },
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pqvk6fcRJvnu"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92G2tnyyKIWE"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTJ_Hxc2UxYR"
   },
   "source": [
    "We require functions capable of converting from row-column based indexing and sequential indexing of grid cells. These are defined below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i3BOKyqxVAer"
   },
   "outputs": [],
   "source": [
    "# Converts row_column format to sequential (state number) format\n",
    "# Input  - 2D array of grid cells in (row, col) format\n",
    "# Output - 1D array of corresponding state numbers\n",
    "def row_col_to_seq(row_col, num_cols): \n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "# Converts sequential (state number) format to row_column format \n",
    "# Input  - 1D array of grid cells in state number format\n",
    "# Output - 2D array of grid cells in corresponding (row, col) format \n",
    "def seq_to_col_row(seq, num_cols): \n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xymLGY_znxPx",
    "outputId": "2d2a7a1f-70c1-44e7-b7b5-757a7fbe40f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_col_to_seq(np.array([[1,0]]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnJXDjixV8pp"
   },
   "source": [
    "## Defining the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXwJVRiaWA8P"
   },
   "source": [
    "The environment class, the definition of which has already been provided as part of the problem statement, is defined here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uzwVett_WwPW"
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "    \n",
    "        # added to help naming conventions\n",
    "        self.wind_mode = None\n",
    "        self.start_mode = None\n",
    "        self.stochasticity = None\n",
    "        \n",
    "        # 'windy' if wind=True | 'clear' if wind=False\n",
    "        self.wind_mode = '_windy' if wind else '_clear'\n",
    "        \n",
    "        # 's1' if starting from [0,4] | 's2' if starting from [3,6] | None otherwise\n",
    "        if (start_state == np.array([[0,4]])).all(): self.start_mode = '_s1'\n",
    "        elif (start_state == np.array([[3,6]])).all(): self.start_mode = '_s2'\n",
    "        \n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "        \n",
    "        # 'detrm' if deterministic (p=1) | 'noisy' if stochastic (p!=1)\n",
    "        self.stochasticity = '_detrm' if (p_good_transition == 1.) else '_noisy'\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "        \n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "                        \n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "        return int(self.start_state_seq)\n",
    "      \n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "            \n",
    "            p += self.P[state, next_state, action]\n",
    "            \n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "            arr = self.P[next_state, :, 3]\n",
    "            next_next = np.where(arr == np.amax(arr))\n",
    "            next_next = next_next[0][0]\n",
    "            return next_next, self.R[next_next]\n",
    "        else:\n",
    "            return next_state, self.R[next_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mbej1hYjaKOF"
   },
   "source": [
    "## Action Policy Definitions\n",
    "\n",
    "This part defines the necessary action selection policies like $\\epsilon$-greedy and softmax. They are used to select the next action of the RL agent based on the state and the Q-values of each action.\n",
    "\n",
    "\n",
    "First, we create an abstract BasePolicy() class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E8wqIujOaWt3"
   },
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'base_policy'\n",
    "\n",
    "    def select_action(self, state_id):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$ - Greedy Policy\n",
    "We first create an epsilon greedy policy which makes use of action value functions as the quantity based on which actions are chosen:\n",
    "\n",
    "\\begin{equation}\n",
    "next\\_action = \n",
    "\\begin{cases} \n",
    "      \\text{argmax}_a(Q(a)) & \\text{with probability }1-\\epsilon \\\\\n",
    "      \\text{random}(a[\\dots]) & \\text{with probability }\\epsilon \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "### Hyperparameter - $\\epsilon$\n",
    "\n",
    "- Gives the ratio of explore to exploit\n",
    "- Higher values mean more exploration of alternate policies\n",
    "- Lower values mean more exploitation of greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Tq9xPdMfdPBJ"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy(BasePolicy):\n",
    "    # epsilon      : epsilon value to be used by epsilon-greedy\n",
    "    # actions      : Possible actions that can be taken up in each state\n",
    "    #              : Default set for the current problem\n",
    "    def __init__(self, epsilon, actions = np.array([0, 1, 2, 3])):\n",
    "        self.eps = epsilon\n",
    "        self.actions = actions\n",
    "        \n",
    "    # return name of policy\n",
    "    @property\n",
    "    def name(self):\n",
    "        return '_eps'\n",
    "\n",
    "    # action_values: 2D array containing predicted action values for each (state, action) pair\n",
    "    #              : array size is |S|*4, i.e, total number of states x total number of actions\n",
    "    # state_id     : state for which action is to be determined\n",
    "    def select_action(self, state_id, action_values):\n",
    "        # To explore or not to explore, that is the question\n",
    "        explore_or_exploit = np.random.binomial(1, 1 - self.eps)\n",
    "\n",
    "        # if exploit, select the arm with maximum value of action value function else choose random arm\n",
    "        if explore_or_exploit == 1:\n",
    "            return self.actions[np.argmax(action_values[state_id, :])]\n",
    "        else:\n",
    "            return np.random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s7QBOBQ1MHr"
   },
   "source": [
    "#### Softmax Policy\n",
    "Next, we create a SoftMax policy for exploration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ihnhSGZMi4P5"
   },
   "outputs": [],
   "source": [
    "class SoftMax(BasePolicy):\n",
    "\n",
    "    # beta      : temperature to be used by softmax function\n",
    "    def __init__(self, beta, actions = np.array([0, 1, 2, 3])):\n",
    "        self.beta = beta\n",
    "        self.actions = actions\n",
    "        \n",
    "    # return name of policy\n",
    "    @property\n",
    "    def name(self):\n",
    "        return '_smx'\n",
    "\n",
    "    # action_values: 2D array containing predicted action values for each (state, action) pair\n",
    "    #              : array size is |S|*4, i.e, total number of states x total number of actions\n",
    "    # state_id     : state for which action is to be determined\n",
    "    def select_action(self, state_id, action_values):\n",
    "\n",
    "        # sample according to the softmax distribution to get an arm with beta temperature\n",
    "        return np.random.choice(self.actions, scipy.special.softmax(action_values[state_id, :]/self.beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXJu3h8L1h5Z"
   },
   "source": [
    "Finally, we also create a greedy deterministic policy which simply chooses the action with maximum Q-value. Although this is not explicitly asked for in the assignment, this can help us quantify how \"good\" the learned policy is which is necessary to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eKFbwBDW14FU"
   },
   "outputs": [],
   "source": [
    "class Greedy(BasePolicy):\n",
    "    # actions      : Possible actions that can be taken up in each state\n",
    "    #              : Default set for the current problem\n",
    "    def __init__(self,  actions = np.array([0, 1, 2, 3])):\n",
    "        self.actions = actions\n",
    "        \n",
    "    # return name of policy\n",
    "    @property\n",
    "    def name(self):\n",
    "        return '_greedy'\n",
    "\n",
    "    # action_values: 2D array containing predicted action values for each (state, action) pair\n",
    "    #              : array size is |S|*4, i.e, total number of states x total number of actions\n",
    "    # state_id     : state for which action is to be determined\n",
    "    def select_action(self, state_id, action_values):\n",
    "\n",
    "        #  select the arm with maximum value of action value function \n",
    "        return self.actions[np.argmax(action_values[state_id, :])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Policy Definitions\n",
    "\n",
    "This part defines the necessary action selection policies like SARSA and QLearning. They are used to update the the Q-values based on the actions of the RL agent and the state and the Q-values of each action.\n",
    "\n",
    "First, we create an abstract BaseUpdate() class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Nyh4ixNDnxP1"
   },
   "outputs": [],
   "source": [
    "class BaseUpdate:\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'base_policy'\n",
    "    \n",
    "    def update(self, state_id):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVbg8OyUl9ki"
   },
   "source": [
    "## SARSA\n",
    "\n",
    "Recall the update rule for SARSA:\n",
    "\\begin{equation}\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "So we have some hyperparameters for the algorithm:\n",
    "- $\\alpha$: learning rate for the RL agent\n",
    "- $\\gamma$: discount factor for values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV-VfCfknCVt"
   },
   "source": [
    "We first define the SARSA update rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m8CshjuJnxP2"
   },
   "outputs": [],
   "source": [
    "class SARSA(BaseUpdate):\n",
    "    # alpha - learning rate\n",
    "    # gamma - discount factor\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    # return name of policy\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'sarsa'\n",
    "    \n",
    "    # q_current - Current estimate for a particular (state,action) pair\n",
    "    # q_future - Future estimates for all actions on a particular state\n",
    "    # reward - reward\n",
    "    # next_action - specifies next action\n",
    "    def update(self, q_current, q_future, reward, next_action = None):\n",
    "        return q_current + self.alpha*(reward + self.gamma*q_future[next_action] - q_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq66-ZzBnxP2"
   },
   "source": [
    "## Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2h4yqBQnxP2"
   },
   "source": [
    "Recall the update rule for Q-Learning:\n",
    "\\begin{equation}\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma \\cdot max_a(Q(s_{t+1}, a)) - Q(s_t, a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "So we have some hyperparameters for the algorithm:\n",
    "- $\\alpha$: learning rate for the RL agent\n",
    "- $\\gamma$: discount factor for values\n",
    "\n",
    "Now we define the QLearning update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "prFswwdNnxP2"
   },
   "outputs": [],
   "source": [
    "class QLearning(BaseUpdate):\n",
    "    # alpha - learning rate\n",
    "    # gamma - discount factor\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    # return name of policy\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'qlrng'\n",
    "    \n",
    "    # q_current - Current estimate for a particular (state,action) pair\n",
    "    # q_future - Future estimates for all actions on a particular state\n",
    "    # reward - reward\n",
    "    # next_action - specifies next action (redundant here)\n",
    "    def update(self, q_current, q_future, reward, next_action = None):\n",
    "        q_future_max = max(q_future)\n",
    "        return q_current + self.alpha*(reward + self.gamma * q_future_max - q_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHGltJcEnxP2"
   },
   "source": [
    "## Iterator for Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfT1vO3p2tFB"
   },
   "source": [
    "The following iterator class forms the crux of the learning process. The class essentially has the ability to perform generalized policy iteration using some update policy and exploration policy. It also has several utility methods to help us visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rToJAJQ7nxP2"
   },
   "outputs": [],
   "source": [
    "class TrainingIterator:\n",
    "    def __init__(self, env, update_policy, exploration_policy, episodes):\n",
    "        self.env = env                         # Grid World Env\n",
    "        self.num_eps = episodes                # training episodes\n",
    "        self.update_policy = update_policy     # SARSA vs Q-Learning\n",
    "        self.explore = exploration_policy      # epsilon-greedy vs softmax\n",
    "        \n",
    "        self.steps = np.zeros(episodes)                                # total_steps for each episode\n",
    "        self.rewards = np.zeros(episodes)                              # total_rewards for each episode\n",
    "        self.q_vals = np.zeros((env.num_states, env.num_actions))      # Qvalues for each (state,action) pair\n",
    "        self.hmap_visits = np.zeros(env.num_states)                    # heatmap of state visits (during training)\n",
    "        self.hmap_qvals = np.zeros(env.num_states)                     # heatmap of Qvalues for optimal actions (after training)\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # iterate over training episodes\n",
    "        for episode in tqdm(range(self.num_eps), desc = 'Training Episodes'):\n",
    "            \n",
    "            current_state  = row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "            current_action = self.explore.select_action(current_state, self.q_vals)\n",
    "\n",
    "            self.steps[episode] = 0\n",
    "            self.rewards[episode] = 0\n",
    "            self.hmap_visits[current_state] += 1\n",
    "\n",
    "            while((current_state not in list(row_col_to_seq(self.env.goal_states, self.env.num_cols))) and (self.steps[episode] <= 100)):\n",
    "\n",
    "\n",
    "                next_state, reward = self.env.step(current_state, current_action)\n",
    "                next_action = self.explore.select_action(next_state, self.q_vals)\n",
    "                self.q_vals[current_state, current_action] = self.update_policy.update(self.q_vals[current_state, current_action], self.q_vals[next_state], reward, next_action)\n",
    "                \n",
    "                if current_state != next_state:\n",
    "                    self.hmap_visits[next_state] += 1\n",
    "                    \n",
    "                current_state = next_state\n",
    "                current_action = next_action\n",
    "                \n",
    "                self.rewards[episode] += reward\n",
    "                self.steps[episode] += 1\n",
    "                \n",
    "            if current_state not in list(row_col_to_seq(self.env.goal_states, self.env.num_cols)):\n",
    "                self.steps[episode] = np.inf\n",
    "\n",
    "                \n",
    "        for state in range(self.env.num_states):\n",
    "            self.hmap_qvals[state] = max(self.q_vals[state])\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def plot_learnt_policy(self, verbose = False):\n",
    "        name = self.update_policy.name + self.env.start_mode + self.env.wind_mode + self.env.stochasticity + self.explore.name + '_lnt_policy.jpg'\n",
    "\n",
    "        greedy_policy = Greedy()\n",
    "        hmap_greedy = np.zeros((self.env.num_rows, self.env.num_cols))\n",
    "        hmap_greedy[self.env.start_state[0][0], self.env.start_state[0][1]] = 1\n",
    "\n",
    "        current_state  = row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "        current_action = greedy_policy.select_action(current_state, self.q_vals)\n",
    "\n",
    "        steps = 0\n",
    "        reward_greedy = 0\n",
    "\n",
    "        while((current_state not in list(row_col_to_seq(self.env.goal_states, self.env.num_cols))) and steps < 100):\n",
    "\n",
    "            next_state, reward = self.env.step(current_state, current_action)\n",
    "            next_action = greedy_policy.select_action(next_state, self.q_vals)    \n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "\n",
    "            reward_greedy += reward\n",
    "            hmap_greedy[seq_to_col_row(current_state, self.env.num_cols)[0][0], seq_to_col_row(current_state, self.env.num_cols)[0][1]] = 1\n",
    "            steps += 1\n",
    "\n",
    "        if verbose:\n",
    "            plt.title(\"Learnt Policy\")\n",
    "            hmap = sns.heatmap(hmap_greedy, annot = False)\n",
    "            plt.savefig('./plots/' + name, pad_inches = 0)\n",
    "            plt.show()\n",
    "        \n",
    "        return reward_greedy[0]\n",
    "    \n",
    "    \n",
    "    def plot_reward_curve(self):\n",
    "        name = self.update_policy.name + self.env.start_mode + self.env.wind_mode + self.env.stochasticity + self.explore.name + '_plt_rewards.jpg'\n",
    "        \n",
    "        sns.set_style(\"darkgrid\")\n",
    "        \n",
    "        plt.title(\"Reward Curve\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.plot(np.arange(self.num_eps), self.rewards, 'b')\n",
    "        \n",
    "        plt.savefig('./plots/' + name, pad_inches = 0)\n",
    "        plt.show()\n",
    "        return\n",
    "        \n",
    "    def plot_steps(self):\n",
    "        name = self.update_policy.name + self.env.start_mode + self.env.wind_mode + self.env.stochasticity + self.explore.name + '_plt_steps.jpg'\n",
    "        \n",
    "        sns.set_style(\"darkgrid\")\n",
    "        \n",
    "        plt.title(\"Steps till Goal State\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Steps\")\n",
    "        plt.plot(np.arange(self.num_eps), self.steps, 'g')\n",
    "        \n",
    "        plt.savefig('./plots/' + name, pad_inches = 0)\n",
    "        plt.show()\n",
    "        return\n",
    "        \n",
    "    def plot_hmap_visits(self):\n",
    "        name = self.update_policy.name + self.env.start_mode + self.env.wind_mode + self.env.stochasticity + self.explore.name + '_hmp_visits.jpg'\n",
    "        \n",
    "        n_rows, n_cols = self.env.num_rows, self.env.num_cols\n",
    "        data = np.zeros((n_rows, n_cols))\n",
    "        for i in range(n_rows):\n",
    "            for j in range(n_cols):\n",
    "                data[i,j] = self.hmap_visits[row_col_to_seq(np.array([[i,j]]), n_cols)[0]]\n",
    "            \n",
    "        plt.title(\"Heatmap of State Visits\")\n",
    "        hmap = sns.heatmap(data, annot = False)\n",
    "        plt.savefig('./plots/' + name, pad_inches = 0)\n",
    "        plt.show()\n",
    "        return\n",
    "        \n",
    "    def plot_hmap_qvals(self):\n",
    "        name = self.update_policy.name + self.env.start_mode + self.env.wind_mode + self.env.stochasticity + self.explore.name + '_hmp_qvals.jpg'\n",
    "        \n",
    "        n_rows, n_cols = self.env.num_rows, self.env.num_cols\n",
    "        data = np.zeros((n_rows, n_cols))\n",
    "        for i in range(n_rows):\n",
    "            for j in range(n_cols):\n",
    "                data[i,j] = self.hmap_qvals[row_col_to_seq(np.array([[i,j]]), n_cols)[0]]\n",
    "            \n",
    "        plt.title(\"Heatmap of Q-Values\")\n",
    "        hmap = sns.heatmap(data, annot = False)\n",
    "        plt.savefig('./plots/' + name, pad_inches = 0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkk7UDaUDELX"
   },
   "source": [
    "In order to carry out the hyperparameter tuning we have decided tune on the basis of their asymptotic optimality, which can be discerned by using the Q_values learnt by the agent and following a greedy action selection mechanism. \n",
    "\n",
    "Next, we define a grid search function which returns the best hyperparameter set based on asymptotic optimality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QH3D3x5HDDiB"
   },
   "outputs": [],
   "source": [
    "def asymptotic_grid_search(env, alphas, gammas, epsilons = None, betas = None, update_rule = 'sarsa'):\n",
    "\n",
    "    best_reward = - np.inf\n",
    "    best_hyper_params_list = []\n",
    "\n",
    "    if epsilons == None:\n",
    "        for alpha in alphas:\n",
    "            for gamma in gammas:\n",
    "                for beta in betas:\n",
    "                    if update_rule == 'sarsa': update_policy = SARSA(alpha, gamma = gamma)\n",
    "                    elif update_rule == 'qlrng': update_policy = QLearning(alpha, gamma = gamma)\n",
    "\n",
    "                    print(\"Current configuration: alpha = {}, gamma = {}, beta = {}\".format(alpha, gamma, beta))\n",
    "                    trainer = TrainingIterator(env, update_policy, SoftMax(beta = beta), 3000)\n",
    "                    trainer.train()\n",
    "\n",
    "                    reward = trainer.plot_learnt_policy()\n",
    "                    print(\"Total Reward:\", reward)\n",
    "\n",
    "                    if reward > best_reward:\n",
    "                        best_hyper_params_list = []\n",
    "\n",
    "                    if reward >= best_reward:\n",
    "                        best_reward = reward\n",
    "                        best_hyper_params = {}\n",
    "                        best_hyper_params['alpha'] = alpha\n",
    "                        best_hyper_params['gamma'] = gamma\n",
    "                        best_hyper_params['beta'] = beta\n",
    "                        best_hyper_params_list.append(best_hyper_params)\n",
    "\n",
    "    else:\n",
    "        for alpha in alphas:\n",
    "            for gamma in gammas:\n",
    "                for epsilon in epsilons:\n",
    "                    if update_rule == 'sarsa': update_policy = SARSA(alpha, gamma = gamma)\n",
    "                    elif update_rule == 'qlrng': update_policy = QLearning(alpha, gamma = gamma)\n",
    "\n",
    "                    print(\"Current configuration: alpha = {}, gamma = {}, epsilon = {}\".format(alpha, gamma, epsilon))\n",
    "                    trainer = TrainingIterator(env, update_policy, EpsilonGreedy(epsilon = epsilon), 3000)\n",
    "                    trainer.train()\n",
    "\n",
    "                    reward = trainer.plot_learnt_policy()\n",
    "                    print(\"Total Reward:\", reward)\n",
    "\n",
    "                    if reward > best_reward:\n",
    "                        best_hyper_params_list = []\n",
    "\n",
    "                    if reward >= best_reward:\n",
    "                        best_reward = reward\n",
    "                        best_hyper_params = {}\n",
    "                        best_hyper_params['alpha'] = alpha\n",
    "                        best_hyper_params['gamma'] = gamma\n",
    "                        best_hyper_params['epsilon'] = epsilon\n",
    "                        best_hyper_params_list.append(best_hyper_params)\n",
    "\n",
    "    print()\n",
    "    print(\"Best Reward\", best_reward)\n",
    "    print(\"Asymptotic Best Hyper Parameters List\", best_hyper_params_list)\n",
    "    print()\n",
    "\n",
    "    return best_reward, best_hyper_params_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the reward obtained may be equal in the asymptotic cases for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret_grid_search(env, best_reward, best_hyper_params, update_rule = 'sarsa', exploration_rule = 'epsilon'):\n",
    "\n",
    "    best_regret = np.inf\n",
    "    best_hyper = {}\n",
    "\n",
    "    if exploration_rule == 'epsilon':\n",
    "        for hyper_dict in best_hyper_params:\n",
    "            \n",
    "            if update_rule == 'sarsa': update_policy = SARSA(alpha = hyper_dict['alpha'], gamma = hyper_dict['gamma'] )\n",
    "            elif update_rule == 'qlrng': update_policy = QLearning(alpha = hyper_dict['alpha'], gamma = hyper_dict['gamma'])\n",
    "\n",
    "            rewards = []\n",
    "            print(\"Current configuration: alpha = {}, gamma = {}, epsilon = {}\".format(hyper_dict['alpha'], hyper_dict['gamma'], hyper_dict['epsilon']))\n",
    "            \n",
    "            for i in range(5):\n",
    "                print(\"Running Experiment:\", i+1)\n",
    "                trainer = TrainingIterator(env, update_policy, EpsilonGreedy(epsilon = hyper_dict['epsilon']), 3000)\n",
    "                trainer.train()\n",
    "                rewards.append(trainer.rewards)\n",
    "                \n",
    "            rewards = np.mean(np.array(rewards), axis = 0)\n",
    "            regret = np.sum((best_reward - rewards))\n",
    "            print(\"Regret:\", regret)\n",
    "\n",
    "            if regret < best_regret:\n",
    "                best_regret = regret\n",
    "                best_hyper['alpha'] =  hyper_dict['alpha']\n",
    "                best_hyper['gamma'] =  hyper_dict['gamma']\n",
    "                best_hyper['epsilon'] =  hyper_dict['epsilon']\n",
    "\n",
    "    else:\n",
    "\n",
    "        for hyper_dict in best_hyper_params:\n",
    "            \n",
    "            if update_rule == 'sarsa': update_policy = SARSA(alpha = hyper_dict['alpha'], gamma = hyper_dict['gamma'] )\n",
    "            elif update_rule == 'qlrng': update_policy = QLearning(alpha = hyper_dict['alpha'], gamma = hyper_dict['gamma'])\n",
    "\n",
    "            rewards = []\n",
    "            print(\"Current configuration: alpha = {}, gamma = {}, beta = {}\".format(hyper_dict['alpha'], hyper_dict['gamma'], hyper_dict['beta']))\n",
    "            \n",
    "            for i in range(5):\n",
    "                trainer = TrainingIterator(env, update_policy, SoftMax(beta = hyper_dict['beta']), 3000)\n",
    "                trainer.train()\n",
    "                rewards.append(trainer.rewards)\n",
    "                \n",
    "            rewards = np.mean(np.array(rewards), axis = 0)\n",
    "            regret = np.sum((best_reward - rewards))\n",
    "            print(\"Regret:\", regret)\n",
    "\n",
    "            if regret < best_regret:\n",
    "                best_regret = regret\n",
    "                best_hyper['alpha'] =  hyper_dict['alpha']\n",
    "                best_hyper['gamma'] =  hyper_dict['gamma']\n",
    "                best_hyper['beta'] =  hyper_dict['beta']\n",
    "\n",
    "    print()\n",
    "    print(\"Best Regret:\", best_regret)\n",
    "    print(\"Best Hyperparameters:\", best_hyper)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    return best_regret, best_hyper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearning - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 3000/3000 [00:09<00:00, 332.67it/s]\n",
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 3000/3000 [00:09<00:00, 317.63it/s]\n",
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 3000/3000 [00:09<00:00, 305.90it/s]\n",
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 3000/3000 [00:08<00:00, 341.28it/s]\n",
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 3000/3000 [00:08<00:00, 335.04it/s]\n",
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 3000/3000 [00:09<00:00, 325.71it/s]\n",
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 3000/3000 [00:08<00:00, 338.26it/s]\n",
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 3000/3000 [00:08<00:00, 337.84it/s]\n",
      "Training Episodes:  46%|███████████████████████████▍                               | 1394/3000 [00:04<00:05, 314.99it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./logs/qlrng_log.txt', 'w') as f:\n",
    "\n",
    "    sys.stdout = f \n",
    "\n",
    "    wind_states = [False, True]\n",
    "    transition_prob = [1.0, 0.7]\n",
    "    start_states = [[0, 4], [3, 6]]\n",
    "    epsilon_soft = ['epsilon','softmax']\n",
    "\n",
    "    \n",
    "\n",
    "    for start in start_states:\n",
    "        for wind_state in wind_states:\n",
    "            for prob in transition_prob:\n",
    "                for exp in epsilon_soft:\n",
    "\n",
    "                    print(\"Running Experiment with parameters: Start State: {}, Wind: {}, Transition Probability: {}, Exploration Policy: {}\".format(start, wind_state, prob, exp))\n",
    "                    print('-------------------------------------------------------------------------------------------------------')\n",
    "                    print()\n",
    "                    num_cols = 10\n",
    "                    num_rows = 10\n",
    "                    obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                                            [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                                            [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                                            [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "                    bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "                    restart_states = np.array([[3,7],[8,2]])\n",
    "                    goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "                    start_state = np.array([start])\n",
    "\n",
    "                    # create model\n",
    "                    gw = GridWorld(num_rows=num_rows,\n",
    "                                    num_cols=num_cols,\n",
    "                                    start_state=start_state,\n",
    "                                    goal_states=goal_states, wind = wind_state)\n",
    "\n",
    "                    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                                        bad_states=bad_states,\n",
    "                                        restart_states=restart_states)\n",
    "\n",
    "                    gw.add_rewards(step_reward=-1,\n",
    "                                    goal_reward=10,\n",
    "                                    bad_state_reward=-6,\n",
    "                                    restart_state_reward=-100)\n",
    "\n",
    "                    gw.add_transition_probability(p_good_transition=prob, bias=0.5)\n",
    "\n",
    "                    env = gw.create_gridworld()\n",
    "\n",
    "                    alphas = [0.001, 0.01, 0.1]\n",
    "                    gammas = [0.5, 0.75, 1]\n",
    "\n",
    "                    if exp == 'epsilon':\n",
    "                        epsilons = [0.001, 0.01, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, epsilons = epsilons, update_rule = 'qlrng')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list,  update_rule = 'qlrng', exploration_rule = 'epsilon' )\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, QLearning(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), EpsilonGreedy(epsilon = best_hyper_params['alpha']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        betas = [1, 5, 10]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, betas = betas, update_rule = 'qlrng')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list, update_rule = 'qlrng', exploration_rule = 'softmax')\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, QLearning(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), SoftMax(beta = best_hyper_params['beta']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA - Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./logs/sarsa_log.txt', 'w') as f:\n",
    "\n",
    "    sys.stdout = f \n",
    "\n",
    "    wind_states = [False, True]\n",
    "    transition_prob = [1.0, 0.7]\n",
    "    start_states = [[0, 4], [3, 6]]\n",
    "    epsilon_soft = ['epsilon','softmax']\n",
    "\n",
    "    \n",
    "\n",
    "    for start in start_states:\n",
    "        for wind_state in wind_states:\n",
    "            for prob in transition_prob:\n",
    "                for exp in epsilon_soft:\n",
    "\n",
    "                    print(\"Running Experiment with parameters: Start State: {}, Wind: {}, Transition Probability: {}, Exploration Policy: {}\".format(start, wind_state, prob, exp))\n",
    "                    print('-------------------------------------------------------------------------------------------------------')\n",
    "                    print()\n",
    "                    num_cols = 10\n",
    "                    num_rows = 10\n",
    "                    obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                                            [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                                            [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                                            [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "                    bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "                    restart_states = np.array([[3,7],[8,2]])\n",
    "                    goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "                    start_state = np.array([start])\n",
    "\n",
    "                    # create model\n",
    "                    gw = GridWorld(num_rows=num_rows,\n",
    "                                    num_cols=num_cols,\n",
    "                                    start_state=start_state,\n",
    "                                    goal_states=goal_states, wind = wind_state)\n",
    "\n",
    "                    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                                        bad_states=bad_states,\n",
    "                                        restart_states=restart_states)\n",
    "\n",
    "                    gw.add_rewards(step_reward=-1,\n",
    "                                    goal_reward=10,\n",
    "                                    bad_state_reward=-6,\n",
    "                                    restart_state_reward=-100)\n",
    "\n",
    "                    gw.add_transition_probability(p_good_transition=prob, bias=0.5)\n",
    "\n",
    "                    env = gw.create_gridworld()\n",
    "\n",
    "                    alphas = [0.001, 0.01, 0.1]\n",
    "                    gammas = [0.5, 0.75, 1]\n",
    "\n",
    "                    if exp == 'epsilon':\n",
    "                        epsilons = [0.001, 0.01, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, epsilons = epsilons, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list,  update_rule = 'sarsa', exploration_rule = 'epsilon' )\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, SARSA(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), EpsilonGreedy(epsilon = best_hyper_params['alpha']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        betas = [1, 5, 10]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, betas = betas, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list, update_rule = 'sarsa', exploration_rule = 'softmax')\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, SARSA(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), SoftMax(beta = best_hyper_params['beta']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ced69849f870846bba8401de8ea27d999c6a9be7d5e22b7f0d1cc9e892f569ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
